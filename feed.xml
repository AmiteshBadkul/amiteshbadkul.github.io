<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://amiteshbadkul.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://amiteshbadkul.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-18T03:49:48+00:00</updated><id>https://amiteshbadkul.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Conformal Prediction In Drug Discovery</title><link href="https://amiteshbadkul.github.io/blog/2025/conformal-prediction/" rel="alternate" type="text/html" title="Conformal Prediction In Drug Discovery"/><published>2025-03-10T13:26:00+00:00</published><updated>2025-03-10T13:26:00+00:00</updated><id>https://amiteshbadkul.github.io/blog/2025/conformal-prediction</id><content type="html" xml:base="https://amiteshbadkul.github.io/blog/2025/conformal-prediction/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>Conformal prediction (CP) is a statistical framework for <em>quantifying uncertainty</em> in machine learning models that provides rigorous confidence guarantees on predictions. Unlike conventional models that output point estimates or uncalibrated probabilities, a conformal predictor produces a <strong>prediction set or interval</strong> that is guaranteed to contain the true value with a user-specified probability (confidence level) under minimal assumptions. For example, at 90% confidence, a CP for regression will output an interval that covers the true experimental value in at least 90% of cases. This property, known as <strong>validity</strong>, holds <em>distribution-free</em> – it does not rely on any specific data distribution or model type, only requiring an exchangeability (roughly, i.i.d.) assumption. In other words, CP offers <em>distribution-agnostic uncertainty quantification</em>: no matter if one uses a random forest or a deep neural network, the CP framework can wrap around it and produce <em>well-calibrated</em> prediction regions with guaranteed coverage. Furthermore, CP is <strong>model-agnostic</strong> and imposes little computational overhead, making it easy to integrate with any machine learning algorithm. These advantages – minimal assumptions, guaranteed <em>marginal coverage</em>, and easy integration – have led to growing interest in CP for high-stakes applications.</p> <p>In drug discovery, the ability to quantify prediction uncertainty is especially crucial. Errors in this domain carry <em>high costs</em>: a false positive (e.g. predicting a molecule will be active when it is not) can waste substantial resources on synthesis and experimental testing, whereas a false negative might cause a promising drug candidate to be overlooked. CP’s guarantee that a specified fraction of predictions will be correct helps limit such false leads. In fact, using CP has been shown to <strong>reduce false positives</strong> and improve the hit rate in virtual screening, since only predictions deemed sufficiently confident are taken as reliable. Drug discovery data also pose challenges like <em>data scarcity</em> and <em>noise</em>. Models are often trained on relatively small, biased datasets – for example, limited assays for a novel target – which restricts their applicability domain. Experimental measurements (e.g. bioassay readouts) can be noisy and variable, adding uncertainty to any predicted potency or ADMET property. CP directly addresses these issues by providing <em>explicit measures of uncertainty</em> alongside predictions, rather than overconfident point estimates. A conformal predictor might output a wide confidence interval for a molecule when data is scarce or noisy, signaling to researchers that the prediction is unreliable and more data or experiments are needed.</p> <p>Another motivation for CP in this field is the increasing emphasis on <strong>trustworthy and explainable AI</strong> in healthcare-related decisions. Regulatory agencies and industry guidelines often require a clear characterization of a model’s domain of applicability and confidence in its predictions. Traditional approaches to define an applicability domain (AD) in QSAR modeling, such as distance-based or ensemble-based heuristics, lack formal statistical guarantees. CP offers a <em>transparent and formal alternative</em> to applicability domain estimation – effectively, each prediction comes with a guarantee of validity under the stated confidence level. Since its introduction to QSAR modeling by Norinder <em>et al.</em> (2014), conformal prediction has been applied in various drug discovery contexts, including virtual screening campaigns, toxicity prediction, and even clinical trial outcome modeling. By producing <em>individualized confidence bounds</em> for each prediction, CP enables more informed decision-making – for instance, prioritizing compounds with both high predicted activity <em>and</em> narrow confidence intervals for synthesis. In summary, CP’s ability to deliver valid prediction sets with guaranteed coverage makes it a powerful tool to increase the <em>reliability and efficiency</em> of drug discovery pipelines, where decisions must carefully balance risk and reward.</p> <h1 id="mathematical-foundations-of-cp">Mathematical Foundations of CP</h1> <p><strong>Exchangeability.</strong> The theoretical guarantee of conformal prediction rests on the assumption of <em>exchangeability</em> of data. A sequence of examples $(z_1, z_2, \dots, z_n)$ (where each $z_i = (x_i, y_i)$ includes features and a label) is <em>exchangeable</em> if its joint probability distribution is invariant under permutation – informally, the data points have no inherent order. Exchangeability is a slightly weaker condition than full independence and identical distribution (i.i.d.): it allows for any ordering of samples as long as there is no temporal or contextual bias. In the drug discovery context, this means we assume that the training, calibration, and test compounds are all drawn from the same underlying distribution (e.g. the same chemical space or experimental protocol). If this assumption holds, no data point carries privileged information (such as being an out-of-distribution example), and the symmetry under permutations can be exploited to derive rigorous confidence statements. <strong>Why is this important?</strong> Under exchangeability, one can prove that CP prediction sets achieve the nominal coverage probability <em>exactly</em> (or conservatively) in finite samples. Essentially, because the new test sample could equally well have appeared as part of the calibration data, its predicted rank among the calibration nonconformity scores is uniformly distributed. This <em>rank invariance</em> implies validity of the CP p-values and prediction sets. If exchangeability is violated – for example, due to <strong>covariate shift</strong> (the test compounds differ systematically from training, as in an evolving chemical library or a new scaffold series) or <strong>temporal drift</strong> (data from a later time have different properties than earlier data) – the CP coverage guarantee may no longer hold. In practice, drug discovery datasets often <em>approximate</em> exchangeability (e.g. when drawing a diverse screening subset at random), but there are notable exceptions. Eklund <em>et al.</em> observed that in a five-year collection of in-house screening data, temporal correlations led to <strong>weakened validity</strong> of conformal predictors (actual coverage fell below nominal). They showed that updating the model and recalibration periodically (a “semi-off-line” CP approach) could partially restore validity. This highlights that careful attention must be paid to data sampling—whenever the i.i.d. assumption is questionable, CP users should check for coverage deviations or use techniques to mitigate dataset shift.</p> <p><strong>Nonconformity Measures.</strong> At the heart of conformal prediction is the concept of a <em>nonconformity measure</em> (also called a <em>strangeness</em> or <em>nonconformity score</em> function). This is a function $A(z)$ that quantifies how “atypical” or <em>nonconforming</em> an example $z = (x,y)$ is relative to a reference set (typically the training or calibration set). Intuitively, $A(z)$ is large if the example does not “fit in” with the others – for instance, if the model’s prediction for $x$ was very different from the true $y$, or if $x$ lies far from the training chemical space. The choice of nonconformity measure is up to the practitioner and serves as a plug-in for domain knowledge. It plays a crucial role: it determines how the model’s errors are judged and thus affects the size (efficiency) of the prediction sets. A simple and common choice in <strong>regression</strong> tasks (e.g. predicting a compound’s $pIC_{50}$ or log-solubility) is the absolute residual error:</p> \[\alpha_i \;=\; A(z_i) \;=\; |y_i - \hat{y}_i|\,,\] <p>where $\hat{y}_i$ is the predicted value for compound $i$ given by a regression model (trained on a proper training set). Intuitively, compounds for which the model makes a large error are considered more nonconforming. In <strong>classification</strong> tasks (e.g. active vs inactive, or multi-class toxicity outcomes), a popular nonconformity score is based on the model’s estimated class probabilities. For an example $z_i=(x_i,y_i)$ with true class $y_i$, one can define:</p> \[\alpha_i = 1 - \hat{P}(y_i \mid x_i)\,,\] <p>i.e. one minus the predicted probability assigned to the true class by the model. This score is near 0 for instances that the model predicts with high confidence (since $\hat{P}(\text{true class})$ is high) and close to 1 for instances the model finds confusing (low predicted probability for the true label). Variants of this for multi-class classification include using the difference between the top predicted probability and that of the true class (a margin-based nonconformity). Many implementations use the <strong>margin</strong> for binary classification; for example, if a Random Forest outputs the fraction of trees voting “active”, one can take $\alpha = 1 - p_{\text{true}}$ (which reduces to 0 if all trees predict the true class, or 0.5 if the model is completely unsure). More generally, any metric of “prediction uncertainty” or “distance from the training data” can serve as a nonconformity measure. This means CP can flexibly incorporate traditional applicability domain metrics: for example, in chemical space modeling one could define $\alpha_i$ as the distance from molecule $x_i$ to its nearest neighbor in the training set, or an ensemble-based uncertainty (such as the entropy of a predictive distribution). <strong>Crucially, the CP validity guarantee holds for <em>any</em> choice of nonconformity function</strong> – the trade-off is that a poor choice can lead to very conservative (wide) prediction sets or reduced efficiency. In practice, one tries to choose $A(z)$ that correlates with prediction error: this yields tighter intervals while still capturing most errors. Nonconformity measures leveraging model-specific information (e.g. the margin from an SVM hyperplane, or attention weights in a graph neural network) can specialize the conformal predictor to the problem at hand.</p> <p><strong>P-Values and Prediction Sets.</strong> Given a nonconformity measure, conformal prediction proceeds by computing <em>p-values</em> for new samples to decide which labels to include in the prediction set. Formally, suppose we have a <strong>calibration set</strong> of $n$ examples (held out and not used in model training) with computed nonconformity scores ${\alpha_1, \alpha_2, \dots, \alpha_n}$. For a new test example with features $x_{n+1}$, and for a candidate label $y$, we first compute the nonconformity score $\alpha_{n+1} = A((x_{n+1}, y))$ by evaluating how strange $(x_{n+1}, y)$ would be relative to the calibration data. In <strong>classification</strong>, this means we temporarily assign the label $y$ to $x_{n+1}$ and compute the score; in <strong>regression</strong>, one typically uses the model’s point prediction $\hat{y}<em>{n+1}$ for $x</em>{n+1}$ and considers potential residuals. The <em>conformal p-value</em> for the new example (with label $y$) is then calculated as:</p> \[p_{y} \;=\; \frac{\#\{ i \in \{1,\dots,n\} : \alpha_i \ge \alpha_{n+1} \} + 1}{\,n+1\,}\,,\] <p>where we add 1 to the count to account for the new sample itself in the ordering. This p-value essentially measures how “compatible” the new example is with the distribution of nonconformity scores seen in calibration – it is the fraction of calibration instances that are at least as strange as the new one (including the new one in the denominator as a random tie-break). A high p-value (close to 1) means the new $(x_{n+1}, y)$ looks very typical (many calibration points have higher nonconformity), whereas a low p-value means $(x_{n+1}, y)$ is an outlier in terms of nonconformity. To make a prediction, we consider the <em>set of labels</em> for which the null hypothesis “the new example conforms as well as the calibration data” is not rejected at the significance level $\varepsilon$. In practice, one chooses a desired <strong>confidence level</strong> $1-\varepsilon$ (e.g. 0.80 or 0.95), which corresponds to an allowed error rate $\varepsilon = 0.20$ or 0.05. The <strong>prediction set</strong> output by a conformal classifier is then:</p> \[\Gamma_{1-\varepsilon}(x_{n+1}) \;=\; \{\,y : p_{y} &gt; \varepsilon \,\}\,,\] <p>the set of all labels that achieve p-value above the significance threshold. This set is guaranteed to contain the true label with probability at least $1-\varepsilon$ (over the randomness of the training/calibration data). In a regression setting where $y$ is a real-valued property, the prediction set $\Gamma$ is typically a continuous interval. A convenient formulation in <em>inductive conformal regression</em> is to use the <em>quantile</em> of calibration residuals: for confidence $1-\varepsilon$, let $Q$ be the $(1-\varepsilon)(n+1)$-th order statistic of ${\alpha_1,\dots,\alpha_n}$ (the $(1-\varepsilon)$ quantile of the calibration errors). Then one can simply set:</p> \[\Gamma_{1-\varepsilon}(x_{n+1}) \;=\; [\,\hat{y}(x_{n+1}) - Q,\;\; \hat{y}(x_{n+1}) + Q\,]\,,\] <p>i.e. an interval around the model’s point prediction, with half-width equal to that quantile. By construction, at most $\varepsilon$ fraction of calibration points had residuals larger than $Q$, so the new interval will fail to cover $y_{n+1}$ at most $\varepsilon$ fraction of the time (in expectation). This simple recipe is often referred to as <em>split conformal prediction</em> in regression settings.</p> <p><strong>CP Variants: Transductive, Inductive, and Split.</strong> There are a few formulations of conformal prediction, differing in how they utilize the data for calibration:</p> <ul> <li> <p><em>Full (Transductive) Conformal Prediction:</em> In the original formulation by Vovk <em>et al.</em>, conformal prediction is applied in a <strong>transductive</strong> or online manner, where each test example is handled one at a time in sequence. For each new point, the model is trained on all available training data, and then for each possible label value, the nonconformity score is computed and compared against all training points (often via a leave-one-out procedure). This yields exact p-values for that test point. Then the test point (with its true label, once revealed) can be added to the set, and the next point is predicted. Transductive CP uses all $n$ training samples plus the test sample in question for calibration, and thus it achieves <em>maximum utilization of data</em> and provably <em>exact</em> validity. However, it is extremely <strong>computationally intensive</strong>: for each test sample and each candidate label, one may need to re-train or at least re-evaluate the model (or nonconformity function) $n+1$ times. In drug discovery problems with large datasets or many test compounds, full CP is often impractical.</p> </li> <li> <p><em>Inductive Conformal Prediction (ICP):</em> To improve efficiency, <em>inductive</em> or <em>split</em> conformal prediction uses a hold-out <strong>calibration set</strong> disjoint from the training set. The model is trained on a <em>proper training set</em> (e.g. 70% of data), and then a separate calibration set (e.g. 20%) is passed through the model to collect nonconformity scores. These calibration scores (together with the exchangeability assumption) serve as a representative distribution for model errors. At prediction time, the model (trained on the proper training set) is applied to a test example <em>once</em> to get the necessary quantities (e.g. predicted label or score), and the calibration scores are used to compute the p-value as described above. In this way, ICP avoids retraining or expensive leave-one-out loops for each test point – the heavy lifting is done only once on the calibration set. The trade-off is that we <em>waste</em> some data for calibration instead of training, potentially reducing model accuracy slightly; but in large datasets this is negligible, and in small data scenarios one can use cross-validation variants. Inductive CP is the most widely used approach in cheminformatics applications because it scales well to modern datasets and integrates neatly with typical train/test splits. Notably, ICP still provides the same validity guarantee: under exchangeability, $\Pr(y_{n+1} \in \Gamma_{1-\varepsilon}(x_{n+1})) \ge 1-\varepsilon$ for any model and any nonconformity function. This guarantee is <em>marginal</em> (averaged over all test points); how to ensure stronger conditional guarantees is a topic of research.</p> </li> <li> <p><em>Split Conformal Prediction:</em> In statistics literature, the term “split conformal” often refers specifically to the inductive method for regression introduced by Papadopoulos <em>et al.</em> and further popularized by Lei <em>et al.</em> – essentially the method of using a calibration set and outputting interval predictions by the quantile method described above. In this article, we treat “split conformal” as synonymous with inductive conformal (since both involve a data split into train and calibration sets). Some authors distinguish them in that ICP computes a p-value for each label and can handle classification with prediction sets, whereas split conformal (sometimes called <em>conformalized quantile regression</em> in modern deep learning contexts) focuses on regression intervals. The underlying theory is the same, with slight differences in implementation. For completeness, we note there are also variants like <em>cross-conformal prediction</em> and <em>jackknife+</em>, which attempt to use <strong>all data for training</strong> while still providing valid intervals, by combining multiple splits or leveraging cross-validation. These are beyond our scope but can be useful for small datasets (common in drug discovery) where one hesitates to sacrifice data for calibration.</p> </li> </ul> <p>In summary, conformal prediction provides a <em>distribution-free, model-agnostic</em> way to obtain <strong>prediction sets with finite-sample guarantees</strong>. The keys are: (1) an exchangeability assumption ensuring the calibration data represent the test distribution, (2) a nonconformity measure measuring how unusual new points are, and (3) the mathematical machinery to convert nonconformity ranks into valid p-values and confidence sets. When these pieces come together, we obtain the powerful result that, for any requested confidence level (say 95%), the method will output a prediction band that contains the true value at least 95% of the time <strong>no matter what the underlying model or data distribution</strong>. This level of reliability is especially appealing in drug discovery applications, as we discuss next.</p> <h1 id="types-of-cp-and-nonconformity-scores-in-drug-discovery">Types of CP and Nonconformity Scores in Drug Discovery</h1> <p>The conformal prediction framework has several <em>extensions and variants</em> that have been tailored to practical considerations in drug discovery. Here we compare some common approaches and discuss how to design nonconformity scores for typical tasks in this domain. <strong>Table 1</strong> provides a summary of key CP variants and their characteristics, followed by a discussion of nonconformity choices for regression vs classification, and specialized data types.</p> <p><strong>CP Variants and Their Trade-offs.</strong> A first important distinction is between <strong>transductive (full) CP</strong> and <strong>inductive CP</strong>, which we introduced above. Transductive CP uses all available labeled data for each prediction and thus achieves the strongest theoretical guarantee (it is provably <em>valid conditionally on the training data</em>, i.e. each test prediction set has exactly $\le \varepsilon$ chance of error given the training set) at the cost of heavy computation. Inductive CP sacrifices some theoretical elegance (validity is marginal over an ensemble of splits) in exchange for <em>efficiency</em>, making it the default for most real-world applications. In the context of drug discovery, where one might need to predict properties for millions of candidate compounds in a virtual screening library, inductive CP is essentially the only feasible option. Another consideration is the handling of <strong>imbalanced data</strong> and <strong>heterogeneous tasks</strong>, which is common in drug discovery (e.g. active compounds are often a tiny fraction of a screening library). A specialized variant called <strong>Mondrian Conformal Prediction (MCP)</strong> addresses this by conditioning the conformal procedure on predefined categories (named after Mondrian’s partitioning). In Mondrian CP, calibration is done <em>within each category or class</em>. For example, in a binary active/inactive classification, one maintains separate calibration score distributions for actives and inactives. This way, one can guarantee <strong>class-conditional validity</strong> – e.g., 90% of active compounds will have their true label in the prediction set at 90% confidence, <em>and</em> 90% of inactive compounds will as well (each class treated independently). This is extremely useful in imbalanced datasets: a regular CP might achieve 90% overall accuracy but perhaps only 50% on the minority class, whereas Mondrian CP ensures the error rate is controlled for each class. The cost is that the calibration data is effectively split by class, so one needs enough data in each stratum to calibrate reliably. In practice, <strong>Inductive Mondrian CP</strong> (combining both strategies) is widely used for classification in drug discovery, as noted by Norinder <em>et al.</em> and others.</p> <p>Other variants worth noting include <strong>cross-validation ensembles</strong> of CP (to use all data without a separate calibration set, at the expense of multiple model fits) and <strong>online CP</strong> for sequential data. In drug discovery, sequential (online) CP could be relevant for <em>active learning</em> or iterative screening settings, where models are updated as new compounds are tested. Researchers have proposed <em>semi-offline CP</em> to handle temporally ordered data, updating the calibration set in batches over time to deal with drift. There are also extensions like <strong>Venn predictors</strong> that output probability distributions with guaranteed calibration, but these are less common in the literature compared to CP and will not be our focus.</p> <p>Table 1 below contrasts some key types of conformal prediction as applied to drug discovery problems:</p> <table> <thead> <tr> <th><strong>CP Variant</strong></th> <th><strong>Description</strong></th> <th><strong>Pros</strong></th> <th><strong>Cons</strong></th> <th><strong>Use in Drug Discovery</strong></th> </tr> </thead> <tbody> <tr> <td><em>Transductive CP</em> (Full)</td> <td>Uses all training data and incorporates each test point on the fly; computes p-value by considering the test point among training points (leave-one-out or retraining for each test).</td> <td>Exact validity for each test sample; uses maximum data for training.</td> <td>Computationally prohibitive for large $n$ or many tests; not practical for high-throughput screening.</td> <td>Rarely used explicitly due to cost; conceptually important as baseline.</td> </tr> <tr> <td><em>Inductive CP</em> (ICP)</td> <td>Splits data into training set and separate calibration set; train once, then calibrate residuals; predict test points without retraining.</td> <td>Efficient (single model training); easy to implement; valid marginal coverage.</td> <td>Uses fewer samples for training (since some are set aside for calibration); validity is marginal, not conditional.</td> <td>Most common approach (e.g. conformal QSAR models. Standard for large-scale predictions.</td> </tr> <tr> <td><em>Mondrian CP</em> (conditional)</td> <td>Calibration is stratified by a context (e.g. class label or data cluster). Each stratum has its own nonconformity distribution.</td> <td>Guarantees coverage <em>within</em> each category (class-conditional validity); handles imbalance better.</td> <td>Requires sufficient calibration data per category; categories must be known upfront (cannot handle totally novel class).</td> <td>Frequently used for classification (active/inactive) and scenarios with known subpopulations (e.g. separate models for assay batches).</td> </tr> <tr> <td><em>Cross-Conformal / Jackknife+</em></td> <td>Averages or combines multiple CP predictors (e.g. via cross-validation) to avoid a single calibration split.</td> <td>Uses all data for training (no data “wasted” purely for calibration); often yields tighter intervals than single-split.</td> <td>More computational cost (train $k$ models for $k$-fold CV); theoretical guarantees become slightly more complex (typically still valid).</td> <td>Used in research when data is very limited (e.g. small <em>in vivo</em> datasets) to maximize data usage. Not yet common in routine practice.</td> </tr> <tr> <td><em>Online/Semi-offline CP</em></td> <td>Continuously update the CP with new data in sequential manner, or periodically recalibrate to account for drift.</td> <td>Adapts to non-stationary data; maintains validity over time if done properly.</td> <td>Implementation complexity; risk of violation of exchangeability if not carefully reset; needs recalibration steps.</td> <td>Applicable in iterative <em>design-make-test</em> cycles and evolving bioassays; Eklund <em>et al.</em> used periodic recalibration for 5-year project data.</td> </tr> </tbody> </table> <p><strong>Nonconformity Measures for Regression vs. Classification.</strong> The choice of nonconformity measure $A(z)$ in a conformal predictor has a large impact on the <em>efficiency</em> (size) of the prediction sets. In drug discovery tasks, one typically leverages the output of the underlying QSAR or ML model to define $A$. We already described common choices: absolute error for regression, and $1 - p(\text{true class})$ for classification. We reiterate these with context:</p> <ul> <li> <table> <tbody> <tr> <td><em>Regression:</em> For a property like binding affinity or pharmacokinetic endpoints (logD, pIC50, etc.), a natural nonconformity score is the difference between predicted and observed value. Often the absolute error $</td> <td>y - \hat{y}</td> <td>$ is used. If the model tends to have larger errors on certain ranges, sometimes scaled residuals or other error metrics can be used. Another strategy is to use <em>quantile regression</em> models to estimate prediction intervals directly, and then apply CP on the quantile regression error – this can yield tighter intervals by incorporating heteroscedasticity (variable noise levels). Simpler yet, one may take the <strong>absolute standardized residual</strong>, if an estimate of uncertainty per compound is available (for example, from an ensemble variance).</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td><em>Classification:</em> For a typical binary classification (active vs inactive, toxic vs non-toxic), one can take the model’s confidence in the predicted class as a measure of conformity. If $\hat{p}$ is the predicted probability for the positive class, a reasonable nonconformity is $\min{\hat{p}, 1-\hat{p}}$ (which is 0.5 for a completely uncertain prediction and near 0 for a very confident prediction). This reduces to $1 - \hat{P}(y_{\text{true}}</td> <td>x)$ as noted earlier. In multi-class settings, a common choice is $1 - \hat{P}(y_{\text{true}}</td> <td>x)$ as well, which again corresponds to the model’s confidence in the true class. Some works instead use the difference between the top predicted probability and the true class probability (so that if the true class is not the top prediction, the score is large). The <strong>fraction of trees voting for the predicted class</strong> in a Random Forest is an example used in early CP papers – effectively an uncalibrated confidence. Notably, if one is using <em>Mondrian CP</em> for class conditioning, the nonconformity measure should be chosen in a <em>label-neutral</em> way (since separate calibration is done per class). In practice, however, many implementations simply use the same formula restricted to each class’s calibration data.</td> </tr> </tbody> </table> </li> <li><em>Hybrid / Other Measures:</em> Some interesting problem-specific nonconformity scores have been proposed in drug discovery. For example, for <strong>graph neural network</strong> models that predict molecular properties, one can design $A(z)$ to incorporate model-internal uncertainty signals: an approach might be to use the <strong>entropy of the model’s output distribution</strong> (for classification) as $\alpha$ (higher entropy = more nonconforming), or use metrics like the <strong>attention weights</strong> in an attentive GNN as a proxy for confidence. As a hypothetical example, one could define $A(z)$ = 1 - <em>(average attention weight on relevant substructure)</em>, so that molecules the GNN pays diffuse attention to are deemed strange. For ligand-based models, the distance in <strong>latent space</strong> (embedding space) to the nearest training compound is a sensible nonconformity measure – this directly connects to applicability domain: a novel chemotype far from any training point should be considered nonconforming (leading to a large prediction set). Indeed, Cortés-Ciriano <em>et al.</em> note that any metric used for applicability domain determination (like similarity or distance) can be plugged into CP as a nonconformity measure. In toxicity prediction, sometimes <em>consensus models</em> are used; one could define $A$ as the <em>disagreement among models</em>, e.g. variance of predictions from an ensemble (with higher variance indicating more nonconformity).</li> </ul> <p>Because nonconformity scores are so flexible, researchers often experiment to find a measure that yields narrow intervals while still maintaining validity. A <strong>poor choice of nonconformity</strong> (for instance, a score uncorrelated with true errors) will still give valid coverage in theory, but may produce unwieldy prediction sets (low efficiency) or flag almost all predictions as “unreliable”. For example, if one chose a completely random nonconformity score, the method would technically cover 90% of true values at 90% confidence, but it would do so by often outputting nearly the entire label space as the prediction set! In contrast, a well-chosen $A(z)$ will differentiate easy-to-predict compounds (low $\alpha$) from hard ones (high $\alpha$), allowing the conformal algorithm to confidently output small sets for the former. There is an inherent <strong>trade-off between validity and efficiency</strong>: CP guarantees validity no matter what $A$ is, but it does not guarantee that the prediction sets will be narrow. The efficiency (average size of intervals or number of labels) depends on how informative the nonconformity measure is about the target. In practice, domain experts use their understanding of the problem to craft $A(z)$ – for instance, incorporating assay noise levels, model calibration information, or known applicability domain limits – to maximize the usefulness of conformal predictions in drug discovery projects.</p> <h1 id="limitations-and-failure-modes-of-cp-in-drug-discovery">Limitations and Failure Modes of CP in Drug Discovery</h1> <p>While conformal prediction is a powerful framework, it is not without limitations, especially when applied to real-world drug discovery data. Understanding when and why CP can fail (or produce less useful results) is key to applying it properly.</p> <p><strong>Violations of Exchangeability:</strong> As discussed, the validity of CP rests on the assumption that the calibration (and training) data and the new samples are exchangeable. In practice, this assumption can be broken in numerous ways in drug discovery. <strong>Covariate shift</strong> is common – for instance, a QSAR model trained on one chemical series may be applied to a different series where the structure–activity relationship is slightly different. Here the new compounds are not drawn from the same distribution as the calibration set. <strong>Temporal drift</strong> is another culprit: models are often built on historical data, then used prospectively. As chemistry efforts progress, later compounds may explore new regions of chemical space or new experimental protocols might alter the data characteristics. Eklund <em>et al.</em> (2015) provide a vivid example: they applied inductive CP to predict assay outcomes over a 5-year project and found the conformal predictor was no longer <em>exactly</em> valid due to time-dependent changes in the data. Specifically, the early data and later data were not exchangeable, which led to actual coverage falling below the nominal level (more errors than expected in later batches). The CP framework does <em>acknowledge</em> this risk – in fact, tools exist to <em>test</em> for exchangeability assumption violations. In the face of drift, one mitigation is to frequently <strong>recalibrate</strong> the CP model. For example, using a sliding window calibration set that always uses the most recent data can help maintain validity at the cost of forgetting old data (semi-offline CP). If distribution shift is suspected (e.g., the model is now being used on a different chemical scaffold or a new patient population), CP predictions should be interpreted with caution. It’s possible in such cases that CP outputs unusually large prediction intervals or empty prediction sets – these are <em>warning signs</em> that the new data may be out of the model’s applicability domain. In summary, CP guarantees are strong but <em>conditional on the data being representative</em>. In drug discovery, where regime changes happen (new target classes, new assay technologies, etc.), one must monitor and adapt the conformal predictor to avoid invalid inference.</p> <p><strong>Choice of Nonconformity Measure:</strong> Although any nonconformity measure yields valid coverage in theory, an <em>inappropriate or poorly tuned $A(z)$</em> can lead to practical failure modes. One issue is <strong>inefficiency</strong>: if the nonconformity scores do not differentiate between good and bad predictions, the resulting prediction sets will be large for almost all compounds. For example, suppose one used a constant nonconformity score for all calibration examples. Then for a new sample, the p-value calculation will basically yield $p \approx \frac{\text{rank}}{n+1}$ based on random tie-breaking – meaning at 80% confidence, roughly 20% of compounds will be randomly declared unreliable, and intervals might be extremely wide to maintain coverage. Such a CP predictor is <em>valid</em> but useless, as it does not meaningfully prioritize compounds. In more realistic terms, if one chooses a nonconformity measure that is only weakly related to prediction error – say, using molecular weight as $A(z)$ when predicting toxicity – the conformal algorithm might still cover the true labels, but the “tightness” of the prediction sets will be suboptimal. Researchers have noted that conformal prediction inherits the weaknesses of the underlying model if not accounted for. For instance, if a QSAR model systematically overfits a particular region of chemistry, the residual-based nonconformity measure might be very small on calibration (if the model fits calibration data too well) but then explode on truly novel compounds, leading to <em>invalid</em> (under-covering) results for those. This ties back to distribution shift as well – a poorly chosen $A(z)$ might not flag novel compounds as strongly as it should. Another subtle issue is <strong>model update frequency</strong>: if one continues to use a fixed calibration set nonconformity distribution while the model parameters themselves are updated (say retrained on more data), the p-values might become mis-calibrated. It is generally required to recalibrate whenever the model changes, otherwise the nonconformity scores no longer reflect the model’s current error behavior.</p> <p>In classification, an interesting failure mode occurs if the conformal prediction sets are always too large or too small. If the model is near-perfect, CP will simply predict a single label for most cases (which is fine). But if the model has blind spots, CP might output prediction sets with multiple labels or even <em>no labels</em> (if all p-values are below $\varepsilon$). The latter case is called a <em>null prediction</em> – it means the algorithm is saying “no label meets the confidence criterion.” This can happen if the new sample has an extremely large nonconformity score, larger than all calibration instances, resulting in $p_{y} = \frac{1}{n+1}$ for all classes $y$. In drug discovery, a null prediction might be interpreted as the compound being outside the model’s domain (none of the model’s predictions can be trusted at the desired level). While this is not a “failure” per se (it is the correct conclusion under the method’s logic), if null predictions occur too frequently, it impedes the usefulness of the model. Mondrian CP can reduce null predictions for minority classes by calibrating their own error rates, but one must still be cautious in very imbalanced cases where even Mondrian calibration data for the rare class is sparse.</p> <p><strong>Computational Challenges:</strong> Inductive conformal prediction is relatively lightweight, but in some scenarios the computation can become non-trivial. One challenge is scaling to <strong>ultra-large datasets</strong>. If one has millions of compounds in a virtual library to evaluate, computing p-values for each can still be time-consuming (since for each test sample, one must compare its nonconformity to $n$ calibration examples). Fortunately, this comparison can be done in $O(n)$ per test or even faster if the nonconformity scores are sorted once. Indeed, many implementations pre-sort the calibration scores and then just find the rank of the new score via binary search, making it very fast. The bigger issue is often <strong>memory</strong>: storing nonconformity scores for very large $n$ or caching model predictions. In large-scale drug discovery projects, one might also use distributed computing to handle CP calculations for many compounds in parallel. Full (transductive) CP is basically infeasible beyond small datasets due to retraining costs, but one could approximate it with clever reuse of computations or incremental training (e.g., for leave-one-out, some ML models like linear models or nearest-neighbors can update predictions quickly without full retrain). Another computational consideration is if one wants <strong>conformal prediction for structured outputs</strong> or complex models – for example, predicting an entire molecular optimization path with confidence. These advanced uses may require custom nonconformity measures and can be expensive to calibrate.</p> <p>In summary, the main failure modes of CP in drug discovery are tied to <em>dataset shift, mis-specified nonconformity measures, and practical computation limits</em>. If exchangeability is violated, the promised coverage can deteriorate – making the CP overly optimistic (dangerously so, if not detected). If the nonconformity metric is not thoughtfully chosen, the CP may be technically valid but provide little value (giant intervals or too many undecided cases). And while CP is computationally cheap relative to many Bayesian uncertainty quantification methods, applying it naïvely in a discovery pipeline with thousands of models or millions of compounds may require optimization. Users of CP should remain vigilant: always verify the empirical coverage on a hold-out if possible, perform checks for changes in data distribution, and refine the nonconformity measure as needed. When done properly, CP will retain its guarantees; when done carelessly, one might be lulled into a false sense of security by “formal” intervals that quietly failed to meet assumptions.</p> <h1 id="future-directions-and-conclusion">Future Directions and Conclusion</h1> <p>Conformal prediction in drug discovery is a rapidly evolving area, and several exciting directions are emerging to further enhance its utility:</p> <p><strong>Integration with Deep Learning Models:</strong> Modern drug discovery increasingly relies on deep learning architectures – from graph neural networks (GNNs) that operate on molecular graphs to transformer models for protein or molecule sequences. Integrating CP with these complex models is a natural next step. Encouragingly, CP is model-agnostic, so in principle one can take a deep model and wrap a conformal calibration around it. Recent studies have done exactly this: Zhang <em>et al.</em> combined deep feed-forward neural nets and GNNs with inductive CP for toxicity prediction (Tox21 datasets). They found that the resulting conformal predictors provided well-calibrated confidence intervals <em>and</em> improved minority-class (toxic) detection compared to the raw models. One reason is that deep learning models often produce overconfident probabilities; conformal calibration adjusts for this, yielding more realistic uncertainty bounds. Moreover, the use of Mondrian CP in conjunction with deep models can tackle class imbalance effectively by enforcing per-class error rates. We anticipate greater exploration of <strong>nonconformity measures tailored to deep models</strong>. For instance, a GNN could use the uncertainty in its node embeddings or the variance in an ensemble of dropouts as part of the nonconformity score. There is also interest in <em>conformal molecular generation</em>: ensuring that when a model generates novel compounds, it can attach confidence that the compound will meet certain property criteria with high probability. Techniques like conformal multi-objective optimization (producing sets of optimized molecules with guaranteed success rates) might appear in the future.</p> <p><strong>Active Learning and Adaptive Experimentation:</strong> Drug discovery is an iterative process – models suggest new compounds, chemists make and test them, and the new data is fed back into models. CP can play a pivotal role in <em>active learning</em> or experiment selection. Because CP provides a principled measure of uncertainty, one can design selection strategies that, for example, prioritize compounds for synthesis which the model is <em>uncertain</em> about (low p-values or very wide prediction intervals) in order to quickly improve the model in those regions. Alternatively, one might focus on compounds that are predicted active with high confidence (small conformal prediction sets) to pursue the most promising leads. Ahlberg <em>et al.</em> (2017) explored strategies to decide which compounds to make next based on conformal predictions of ADME properties. They compared different automated decision rules (like focusing on high-confidence positives vs exploring low-confidence regions) and demonstrated that using CP-informed criteria can save experimental resources while still finding optimal drug candidates. We foresee CP becoming a standard component in <strong>design–make–test–analyze (DMTA)</strong> cycles, as a decision support tool: for example, flagging which predicted leads are <em>within</em> the model’s confidence domain and which are extrapolations. CP can also enhance <strong>Bayesian optimization</strong> for drug design by ensuring that the model’s proposed next candidates meet a desired confidence level.</p> <p><strong>Beyond Marginal Coverage – Conditional and Localized Validity:</strong> One active research area in conformal prediction is improving the <em>conditional</em> validity of CP. The standard CP guarantee is <em>marginal</em>, meaning averaged over all samples the error rate is bounded by $\varepsilon$. In drug discovery, one might desire stronger guarantees for certain subsets – e.g., “This model’s activity predictions are 90% accurate <em>for compounds similar to our lead series</em>.” Mondrian CP is one approach to conditional validity based on known categories, but there is ongoing work on more flexible conditioning (like conditioning on a continuous descriptor). Methods like <strong>conditional conformal prediction</strong> and weighting schemes could allow CP to maintain validity on important subdomains (such as a particular chemistry or a particular range of property values). This might involve training a separate conformal predictor for, say, large molecules vs small molecules, or using covariate-dependent nonconformity functions. While not trivial, any advances here would directly benefit drug discovery by allowing more nuanced risk assessments – for instance, knowing that within a well-explored scaffold series the CP is tight and valid, whereas in a novel series it defaults to broader, more conservative predictions.</p> <p><strong>User Interpretability and Trust:</strong> As CP becomes more integrated into drug discovery pipelines, an interesting “soft” aspect is how medicinal chemists and project teams interact with these predictions. The presentation of conformal prediction sets (e.g., “Compound X will have pIC50 between 6.5 and 7.8 with 90% confidence”) needs to be communicated effectively to non-experts. Visualization tools, such as plotting the predicted interval for each compound alongside actual values, can help build intuition. Trust in AI models can be significantly bolstered by methods like CP: knowing that a model can <em>say “I’m not sure”</em> for certain compounds is comforting in a decision-making context. There is potential to combine CP with <strong>explainable AI techniques</strong> – for example, providing an explanation for why a prediction interval is wide (perhaps highlighting a molecular substructure that lies outside the training domain).</p> <p><strong>Automating Nonconformity Selection and Efficiency Improvements:</strong> Another future direction is to automate or learn the nonconformity measure itself. Instead of manually specifying $A(z)$, one could imagine a meta-learning approach where we train a small model to predict the error of the main model, and use that as $A$. Ensemble and stacking approaches already hint at this (where an ensemble’s disagreement correlates with error). Research into <em>adaptive conformal prediction</em> aims to make prediction sets smaller while preserving validity, by using information about $x$ (features) in the calibration – essentially trying to achieve conditional validity. This might involve training models that output <em>interval predictions</em> directly (like quantile regression or Bayesian NNs) and then conformalizing them (a technique known as Conformalized Quantile Regression, CQR). Such approaches could yield <strong>tighter intervals for easy compounds</strong> and only wide intervals for genuinely hard cases.</p> <p>In conclusion, conformal prediction has emerged as a valuable tool to provide rigorous uncertainty quantification in drug discovery and chemical machine learning. Its ability to deliver <em>valid confidence measures</em> for each prediction addresses a critical need in a field where decisions can have large financial and scientific consequences. By outputting prediction sets with a guaranteed coverage probability, CP methods allow researchers to identify which predictions can be trusted and which require caution. This leads to more efficient use of resources – focusing experiments where the model is less certain, and expediting projects when the model is confident. We have discussed how CP operates, its mathematical underpinnings (exchangeability and nonconformity), and how it can be adapted (inductive, Mondrian, etc.) for practical use. We also examined limitations like dataset shift and the importance of proper calibration. Going forward, the integration of CP with state-of-the-art deep learning and its deployment in active learning loops hold great promise for making drug discovery AI both <strong>reliable and informative</strong>. As the industry embraces data-driven decisions, methods like conformal prediction will be instrumental in ensuring that those decisions are backed by sound uncertainty estimates – ultimately increasing the <em>trust</em> in AI models to guide the discovery of new therapeutics.</p> <p><strong>References:</strong></p> <ol> <li> <p>Shafer, G. &amp; Vovk, V. (2008). <em>A Tutorial on Conformal Prediction</em>. <strong>Journal of Machine Learning Research, 9</strong>, 371–421.</p> </li> <li> <p>Cortés-Ciriano, I. &amp; Bender, A. (2019). <em>Concepts and Applications of Conformal Prediction in Computational Drug Discovery</em>. <strong>Molecular Informatics, 39</strong>(8-9), 1900351.</p> </li> <li> <p>Norinder, U. <em>et al.</em> (2014). <em>Introducing Conformal Prediction in Predictive Modeling. A Transparent and Flexible Alternative to Applicability Domain Determination</em>. <strong>J. Chem. Inf. Model., 54</strong>(6), 1596–1603.</p> </li> <li> <p>Eklund, M., Norinder, U., Boyer, S. &amp; Carlsson, L. (2015). <em>The application of conformal prediction to the drug discovery process</em>. <strong>Ann. Math. Artif. Intell., 74</strong>, 117–132.</p> </li> <li> <p>Zhang, J. <em>et al.</em> (2021). <em>Deep Learning Based Conformal Prediction of Toxicity</em>. <strong>J. Chem. Inf. Model., 61</strong>(6), 2641–2653.</p> </li> <li> <p>Ahlberg, E. <em>et al.</em> (2018). <em>Using Conformal Prediction to Prioritize Compound Synthesis in Drug Discovery</em>. <strong>Proc. of Machine Learning Research, 60</strong>, 29–48.</p> </li> <li> <p>Lei, J., G’Sell, M., Rinaldo, A., Tibshirani, R. &amp; Wasserman, L. (2018). <em>Distribution-Free Predictive Inference for Regression</em>. <strong>Journal of the American Statistical Association, 113</strong>(523), 1094–1111.</p> </li> <li> <p>Vovk, V., Gammerman, A. &amp; Shafer, G. (2005). <em>Algorithmic Learning in a Random World</em>. <strong>Springer</strong>.</p> </li> <li> <p>Papadopoulos, H. (2008). <em>Inductive Conformal Prediction: Theory and Application to Neural Networks</em>. In <strong>Tools in Artificial Intelligence</strong>, ed. by D. T. Larose, pp. 315–330.</p> </li> <li> <p>Linusson, H., Boström, H. &amp; Johansson, U. (2014). <em>Different approaches to address the class imbalance problem in conformal prediction</em>. <strong>Proc. of COPA 2014</strong>, 11–19.</p> </li> </ol>]]></content><author><name></name></author><category term="literature-review"/><category term="uncertainty"/><category term="ml"/><category term="dl"/><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Analysis of HP1α interactions</title><link href="https://amiteshbadkul.github.io/blog/2023/hp1-protein/" rel="alternate" type="text/html" title="Analysis of HP1α interactions"/><published>2023-08-20T15:59:00+00:00</published><updated>2023-08-20T15:59:00+00:00</updated><id>https://amiteshbadkul.github.io/blog/2023/hp1-protein</id><content type="html" xml:base="https://amiteshbadkul.github.io/blog/2023/hp1-protein/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>This article seeks to review the seminal work conducted by Her et al. (2022), which has significantly advanced our understanding of HP1α and its interactions. All figures included in this article are sourced from the original paper by Her et al. (2022). Full citation details can be found in the References section.</p> <h3 id="brief-on-heterochromatin">Brief on Heterochromatin</h3> <p>Heterochromatin is a tightly packed form of DNA, distinguished from the more relaxed structure of euchromatin. This compact organization serves as a regulatory mechanism for gene expression. Due to its dense structure, genes located within heterochromatic regions are generally less accessible to the cellular machinery responsible for transcription. This inaccessibility often leads to the repression or silencing of these genes. The tightly packed nature of heterochromatin creates a physical barrier that hinders the binding of transcription factors and RNA polymerases, thereby reducing the likelihood of gene expression. Understanding the mechanisms that govern the formation and maintenance of heterochromatin is crucial for a comprehensive grasp of gene regulation, cellular differentiation, and even disease progression.</p> <h3 id="introduction-to-hp1α">Introduction to HP1α</h3> <p>HP1α (Heterochromatin Protein 1 alpha) is a chromosomal protein that plays a pivotal role in the formation and maintenance of heterochromatin. It is one of the three mammalian HP1 isoforms, the others being HP1β and HP1γ. HP1α is particularly significant for its role in gene silencing, a function it performs by binding to specific epigenetic markers on histones, such as H3K9me3 (trimethylated lysine 9 on histone H3). Once bound, HP1α can recruit other proteins that contribute to the compact, repressive structure of heterochromatin.</p> <h3 id="role-in-chromatin-organization">Role in Chromatin Organization</h3> <p>HP1α is not just a passive component of heterochromatin; it actively contributes to its organization. By interacting with other proteins and with the DNA itself, HP1α helps to create and stabilize the tightly packed heterochromatin structure. This, in turn, contributes to the repression of gene expression by making the DNA less accessible to the transcriptional machinery.</p> <h3 id="significance-in-gene-silencing">Significance in Gene Silencing</h3> <p>The ability of HP1α to bind to specific regions of the chromatin and recruit other proteins makes it a key player in gene silencing. Its role is especially critical in maintaining the integrity of heterochromatic regions, ensuring that genes within these areas remain inactive. This has broader implications for cellular differentiation, development, and disease, as aberrant gene expression can lead to a variety of pathological conditions.</p> <h3 id="need-for-the-study">Need for the Study</h3> <p>HP1α is a pivotal player in chromatin organization and gene regulation, yet there are significant gaps in our understanding of its molecular behavior. One of the most intriguing aspects that remain poorly understood is how HP1α undergoes liquid-liquid phase separation (LLPS) to form biomolecular condensates within cells. These condensates are not just cellular curiosities; they play a vital role in compartmentalizing cellular processes, thereby influencing gene regulation, RNA processing, and other essential cellular functions. Therefore, understanding the factors that influence HP1α’s propensity for LLPS is not just an academic exercise but a necessity for grasping its role in cellular function and disease states.</p> <h3 id="liquid-liquid-phase-separation-llps">Liquid-Liquid Phase Separation (LLPS)</h3> <p>LLPS is a biophysical phenomenon where a homogeneous solution spontaneously separates into two co-existing liquid phases: one dense and one dilute. In cellular biology, LLPS is increasingly recognized as a fundamental mechanism that underpins the formation of membrane-less organelles and other functional compartments within cells. These compartments serve as reaction hubs or storage depots for various biomolecules. Understanding the LLPS behavior of HP1α is therefore critical for gaining insights into its role in chromatin organization, and by extension, its impact on gene regulation and cellular function.</p> <h3 id="phosphorylation">Phosphorylation</h3> <p>Phosphorylation is a type of post-translational modification where a phosphate group is covalently attached to an amino acid residue in a protein. This modification often leads to a change in the protein’s function, stability, or interaction with other molecules. In the context of HP1α, phosphorylation has been shown to significantly affect its conformation and its propensity for LLPS. This makes the study of phosphorylation essential for a comprehensive understanding of HP1α’s role in chromatin dynamics, as it could reveal new avenues for therapeutic interventions targeting gene regulation.</p> <h2 id="structure-of-hp1α">Structure of HP1α</h2> <p>HP1α is a highly conserved protein with a modular architecture, consisting of an N-terminal extension (NTE), a chromodomain (CD), a hinge region, and a chromoshadow domain (CSD). Each of these domains plays a specific role in HP1α’s function and its interactions with other molecules. The chromodomain is primarily responsible for binding to methylated histones, thereby localizing HP1α to specific regions of chromatin. The hinge region serves as a flexible linker and also participates in DNA and RNA binding. The chromoshadow domain is involved in dimerization and interactions with other proteins.</p> <table> <thead> <tr> <th style="text-align: center"><img src="https://imgur.com/EkNZpBE.png" width="75%" height="75%"/></th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><em>The structure and sequence of HP1α</em></td> </tr> </tbody> </table> <h3 id="contributions-of-the-paper">Contributions of the Paper</h3> <ul> <li> <p>Multi-Scale Simulations: The paper employs a multi-scale approach, utilizing atomistic, coarse-grained, and coarse-grained coexistence simulations to provide a comprehensive understanding of HP1α’s behavior.</p> </li> <li> <p>Experimental Validation: The study incorporates experimental techniques like small-angle X-ray scattering and fluorescence microscopy to validate the computational findings, thereby strengthening the reliability of the results.</p> </li> <li> <p>Role of Phosphorylation: One of the key contributions is the elucidation of how phosphorylation modulates HP1α’s conformation and its propensity for liquid-liquid phase separation (LLPS).</p> </li> <li> <p>Influence of DNA: The paper investigates how DNA affects HP1α’s phase behavior, providing insights into its role in chromatin organization.</p> </li> <li> <p>Ligand Effects: The study also explores the impact of ligands on HP1α’s LLPS, emphasizing the role of charge over binding affinity.</p> </li> <li> <p>Multi-Dimensional View: By examining the influence of multiple factors like phosphorylation, DNA, and ligands, the paper offers a multi-dimensional perspective on the regulation of HP1α’s function in chromatin organization.</p> </li> </ul> <h2 id="simulations">Simulations</h2> <p>The study’s strength lies in its multi-scale, multi-level simulation approach. By employing atomistic simulations for detailed insights, coarse-grained simulations for a broader view, and coarse-grained coexistence simulations for phase behavior, the researchers achieved a comprehensive understanding of HP1α’s complex behavior. This multi-faceted approach allowed them to validate their findings at multiple levels, lending robustness and credibility to their conclusions. It is a commendable strategy that sets a precedent for future studies in the field.</p> <h3 id="atomistic-simulations">Atomistic Simulations</h3> <ul> <li> <p>Methodology: Atomistic simulations offer a granular, high-fidelity representation of molecular interactions. While computationally demanding, they provide unparalleled insights into the specific forces and interactions at play, down to individual atoms.</p> </li> <li> <p>Relevance to HP1α: In the study of HP1α, atomistic simulations were invaluable for dissecting the intricate electrostatic and van der Waals forces that govern its interactions with DNA and ligands. These simulations allowed the researchers to identify specific residues and domains critical for phase separation, thereby offering a nuanced understanding that would be difficult to achieve otherwise.</p> </li> </ul> <h3 id="coarse-grained-simulations">Coarse-Grained Simulations</h3> <ul> <li> <p>Methodology: Coarse-grained simulations are a computational shortcut that simplifies the molecular system by representing clusters of atoms as single interaction points. This enables the study of larger systems over extended timescales, albeit at the cost of some molecular detail.</p> </li> <li> <p>Relevance to HP1α: For HP1α, coarse-grained simulations served as a bridge between atomistic detail and system-level behavior. They allowed the researchers to simulate larger assemblies of HP1α and its interactions with DNA, providing a broader view of its phase behavior and the impact of phosphorylation and ligand binding.</p> </li> </ul> <h3 id="coarse-grained-coexistence-simulations">Coarse-Grained Coexistence Simulations</h3> <ul> <li> <p>Methodology: These are a specialized subset of coarse-grained simulations designed explicitly for studying the coexistence of different phases. They are particularly useful for understanding phenomena like phase separation.</p> </li> <li> <p>Relevance to HP1α: These simulations were crucial for studying the liquid-liquid phase separation (LLPS) of HP1α in various conditions. They provided a comprehensive understanding of how phase separation is influenced by factors like DNA and ligands, and under what conditions it occurs.</p> </li> </ul> <table> <thead> <tr> <th style="text-align: center"><img src="https://imgur.com/mBzOFmJ.png" width="75%" height="75%"/></th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><em>Droplet Formation for each simulation</em></td> </tr> </tbody> </table> <h2 id="conclusions-from-simulations">Conclusions from Simulations</h2> <h3 id="radius-of-gyration">Radius of Gyration</h3> <ul> <li>Implications for HP1α and pHP1α: The radius of gyration (Rg) is a measure of how extended a molecule is in space. In the context of HP1α, a smaller Rg indicates a more compact structure. This compactness is especially pronounced in its phosphorylated form, pHP1α. The compact structure is not just a molecular curiosity; it has functional implications. A more compact HP1α can bind to DNA more efficiently, leading to the formation of heterochromatin, a tightly packed form of DNA. This tight packing makes the DNA less accessible for transcription, effectively repressing gene expression.</li> </ul> <h3 id="charges">Charges</h3> <ul> <li>Role in LLPS: Electrostatic interactions are a significant driver for the liquid-liquid phase separation (LLPS) of HP1α. The study meticulously showed that the negatively charged N-terminal extension (NTE) and the positively charged hinge region of HP1α are crucial for this. Phosphorylation adds another layer of complexity by introducing additional negative charges, making the electrostatic landscape even more conducive for LLPS. Ligands that can neutralize these charges play a role in modulating this phase separation, emphasizing the delicate balance of electrostatic forces in biological systems.</li> </ul> <h3 id="contact-map">Contact Map</h3> <p>Intermolecular van der Waals Contacts: The contact map is a powerful tool that provides a graphical representation of the spatial proximity between amino acid residues in a protein. In the context of this study, the contact map was employed to visualize van der Waals interactions between residues of HP1α and its binding partners, including DNA. This level of detail is crucial for understanding the specific molecular interactions that drive HP1α’s ability to undergo liquid-liquid phase separation (LLPS). By identifying key residues that are in close contact during LLPS, the study provides a molecular-level understanding of how HP1α molecules interact with each other and with DNA. This is a vital piece of the puzzle for understanding the mechanisms that lead to chromatin compaction and gene regulation.</p> <table> <thead> <tr> <th style="text-align: center"><img src="https://imgur.com/hsL7RxE.png" width="75%" height="75%"/></th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><em>Contact Map</em></td> </tr> </tbody> </table> <h3 id="saturation-concentrations">Saturation Concentrations</h3> <p>Csat and LLPS: Saturation concentration (Csat) is a critical parameter that indicates the concentration of protein in the dilute phase before LLPS occurs. In the study, it was observed that both ligands and phosphorylation states could modulate this concentration. For instance, positively charged ligands like Sgo1 and H3 peptides were found to lower the saturation concentration, thereby enhancing LLPS. This is a significant finding as it suggests that cellular mechanisms could control chromatin organization by modulating the saturation concentration of HP1α. Understanding how Csat is influenced by various factors provides another layer of complexity and control in cellular gene regulation mechanisms.</p> <h3 id="dna-interactions">DNA Interactions</h3> <p>Key Residues and Chromatin Organization: The study didn’t just focus on protein-protein interactions; it also explored the key residues in HP1α that interact with DNA. This is particularly important because HP1α’s role in chromatin organization is not just a function of its ability to undergo LLPS, but also its ability to interact with DNA in a specific manner. By identifying these key residues, the study provides insights into the molecular mechanisms through which HP1α contributes to chromatin compaction. This, in turn, has implications for how genes are regulated, as chromatin structure is a key factor in gene accessibility for transcription machinery.</p> <h2 id="effects-on-llps-of-hp1α">Effects on LLPS of HP1α</h2> <h3 id="phosphorylation-1">Phosphorylation</h3> <ul> <li>Altering LLPS Dynamics: Phosphorylation, the enzymatic addition of a phosphate group to a protein, was found to significantly influence HP1α’s behavior in liquid-liquid phase separation (LLPS). The study meticulously demonstrated that phosphorylation introduces negative charges to the HP1α molecule, thereby making its dense phase highly negatively charged. This is a groundbreaking revelation as it suggests that post-translational modifications like phosphorylation can serve as cellular regulatory mechanisms to control chromatin organization. The implications of this are far-reaching, potentially affecting everything from gene expression to cellular differentiation.</li> </ul> <table> <thead> <tr> <th style="text-align: center"><img src="https://imgur.com/Pvmfgh9.png" width="75%" height="75%"/></th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><em>Phosphorylation Process</em></td> </tr> </tbody> </table> <h3 id="dna">DNA</h3> <ul> <li>Role in Phase Separation: The study went to great lengths to examine how DNA contributes to HP1α LLPS. The DNA used in the experiments was carefully designed to mimic the repetitive nature of heterochromatic regions, thereby allowing for multiple HP1α binding sites. This is a critical aspect for understanding how HP1α interacts with chromatin in a cellular context. Furthermore, the DNA length was optimized to allow the formation of phase-separated condensates rather than just 1:1 complexes. This provides a more realistic model of chromatin organization, capturing the complex interactions between HP1α and DNA.</li> </ul> <h3 id="ligandspeptides">Ligands/Peptides</h3> <p>Charge Over Binding Affinity: One of the most striking findings of the study was the role of peptide ligands in modulating HP1α LLPS. Through a series of well-designed experiments and simulations, the study found that positively charged ligands, such as Sgo1 and H3 peptides, enhanced the phase separation process. In contrast, negatively charged ligands disrupted it. Intriguingly, this effect was observed even in the absence of specific binding interactions between the ligands and HP1α. This suggests that the electrostatic charge of the ligand is a more important driver in LLPS than the specific binding affinity. This opens up new avenues for understanding how various cellular components, including but not limited to ligands, can modulate chromatin structure and function.</p> <table> <thead> <tr> <th style="text-align: center"><img src="https://imgur.com/IvWwwj1.png" width="75%" height="75%"/></th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><em>Ligand/Peptide Interactions</em></td> </tr> </tbody> </table> <h2 id="limitations">Limitations</h2> <p>The study, despite its limitations, sets a strong foundation for future research. Its multi-level, multi-scale approach is particularly commendable, providing a nuanced understanding that is often lacking in studies of biological systems. The use of a DNA fragment that captures some of the complexity of chromatin interactions is also praiseworthy, even as we acknowledge the limitations of this approach.</p> <h3 id="in-vitro-vs-in-vivo-differences">In-vitro vs In-vivo Differences</h3> <ul> <li>Simplification of Cellular Complexity: The study’s reliance on simplified in-vitro systems is a double-edged sword. While these systems offer the advantage of isolating specific molecular interactions for detailed study, they lack the intricate cellular environment that would naturally influence HP1α’s behavior. This absence of cellular complexity means that the study’s findings, although robust within their scope, may not fully translate to the in-vivo context. For example, the in-vivo environment would include a myriad of other proteins, RNAs, and cellular structures that could interact with HP1α, potentially altering its phase separation behavior. Therefore, future research should aim to incorporate these complexities to validate the study’s findings in a more biologically relevant setting.</li> </ul> <h3 id="use-of-short-linear-dna-fragments">Use of Short Linear DNA Fragments</h3> <ul> <li>Lack of Native Chromatin Structure: The study employed 205 bp linear DNA fragments as a proxy for the repetitive nature of heterochromatic regions. While this design choice allows for multiple HP1α binding sites, it does not capture the three-dimensional structure and complexity of native chromatin. Chromatin in cells is not merely a linear sequence but a dynamic and complex structure that can influence protein interactions and thus, phase separation. The authors themselves acknowledge this limitation, suggesting that future work could benefit from using more complex chromatin substrates, possibly even native chromatin extracted from cells, to better understand HP1α’s role in chromatin organization.</li> </ul> <h3 id="hp1β-and-hp1γ-paralogs">HP1β and HP1γ Paralogs</h3> <ul> <li>Narrow Focus on HP1α: The study provides a comprehensive analysis of HP1α but does not extend this scrutiny to its paralogs, HP1β and HP1γ. These paralogs are not just molecular mimics of HP1α; they have unique roles in chromatin dynamics and gene regulation, including both gene activation and silencing. Their exclusion from the study leaves a gap in our understanding of how the HP1 family as a whole contributes to chromatin organization and gene regulation. Future studies should aim to include these paralogs to provide a more holistic view of HP1 function in chromatin dynamics.</li> </ul> <h2 id="future-directions">Future Directions</h2> <p>Each of these future directions not only builds upon the current study’s findings but also opens new doors for research, offering exciting possibilities for advancing our understanding of chromatin dynamics and gene regulation.</p> <h3 id="complex-cellular-environment-modeling">Complex Cellular Environment Modeling</h3> <p>Incorporation of Cellular Complexity: One of the most promising avenues for future research is to simulate or experimentally validate HP1α’s behavior in a more complex cellular environment. This would involve not just the addition of other proteins but also RNAs, lipids, and other cellular structures that could interact with HP1α. Such a comprehensive model would provide insights into how HP1α behaves in the context of a living cell, thereby making the findings more biologically relevant. This could involve advanced computational models that incorporate these factors or more complex in-vitro systems that mimic the cellular environment more closely.</p> <h3 id="study-of-hp1-paralogs">Study of HP1 Paralogs</h3> <p>Comprehensive Understanding of Chromatin Dynamics: While the study provides a detailed analysis of HP1α, it leaves a gap in our understanding of the HP1 family as a whole. Future research should aim to include HP1β and HP1γ to provide a more holistic view. This would not only deepen our understanding of chromatin organization but also potentially reveal how these paralogs might interact with each other and with HP1α to regulate gene expression.</p> <h3 id="chromatin-organization-and-gene-regulation-in-therapeutic-context">Chromatin Organization and Gene Regulation in Therapeutic Context</h3> <p>Therapeutic Implications of HP1α and Ligands: Given HP1α’s role in chromatin organization and, by extension, gene regulation, understanding its behavior opens the door for therapeutic interventions. This is particularly relevant in diseases like cancer, where gene expression is often dysregulated. The study’s findings on how ligands and phosphorylation affect HP1α’s phase separation offer a tantalizing glimpse into how these processes could be manipulated for therapeutic purposes. For instance, specific ligands could be designed to either enhance or inhibit HP1α’s phase separation, thereby affecting chromatin structure and gene expression in a controlled manner. This could lead to the development of novel drugs or therapeutic practices that target these specific mechanisms.</p> <h2 id="conclusion">Conclusion</h2> <p>The study under review offers a comprehensive and multi-faceted exploration into the behavior of HP1α, a key player in chromatin organization and gene regulation. Utilizing a blend of atomistic, coarse-grained, and coarse-grained coexistence simulations, complemented by experimental validation, the research provides valuable insights into HP1α’s liquid-liquid phase separation (LLPS) and its modulation by phosphorylation, DNA, and ligands.</p> <p>While the study is thorough in its approach, employing multi-level simulations to capture the complex interactions at play, it is not without limitations. The use of simplified in-vitro systems and short linear DNA fragments, although useful for isolating specific behaviors, does not fully capture the complexity of the cellular environment. Additionally, the focus on HP1α leaves questions about the roles of its paralogs, HP1β and HP1γ, unanswered.</p> <p>Despite these limitations, the study sets the stage for future research in several promising directions. These include the incorporation of more complex cellular environments in simulations, the inclusion of HP1 paralogs to provide a more comprehensive understanding of chromatin dynamics, and the exploration of therapeutic interventions targeting HP1α’s role in chromatin organization and gene regulation.</p> <p>In summary, the paper makes a significant contribution to our understanding of HP1α and its role in LLPS, laying a strong foundation for future studies that could have far-reaching implications in both basic biology and therapeutic applications.</p> <h2 id="acknowledgment">Acknowledgment</h2> <h2 id="references">References</h2> <ol> <li><a href="https://doi.org/10.1093/nar/gkac1194">Molecular interactions underlying the phase separation of HP1α: role of phosphorylation, ligand and nucleic acid binding, Nucleic Acids Research</a></li> <li><a href="https://pubmed.ncbi.nlm.nih.gov/28636604/">Liquid droplet formation by HP1α suggests a role for phase separation in heterochromatin</a></li> <li><a href="https://elifesciences.org/articles/64563">HP1 proteins compact DNA into mechanically and positionally stable phase separated domains</a></li> <li><a href="https://www.nature.com/articles/s41392-021-00678-1">Liquid–liquid phase separation in human health and diseases</a></li> </ol>]]></content><author><name></name></author><category term="literature-review"/><category term="literature"/><category term="md"/><summary type="html"><![CDATA[Analysis of "Molecular interactions underlying the phase separation of HP1α. Role of phosphorylation, ligand and nucleic acid binding, Nucleic Acids Research" Publication]]></summary></entry><entry><title type="html">Evolutionary Scale Modeling using Protein Language Models</title><link href="https://amiteshbadkul.github.io/blog/2023/esm2-explained/" rel="alternate" type="text/html" title="Evolutionary Scale Modeling using Protein Language Models"/><published>2023-07-29T15:59:00+00:00</published><updated>2023-07-29T15:59:00+00:00</updated><id>https://amiteshbadkul.github.io/blog/2023/esm2-explained</id><content type="html" xml:base="https://amiteshbadkul.github.io/blog/2023/esm2-explained/"><![CDATA[<h2 id="language-models-in-biological-domain">Language Models in Biological Domain</h2> <p>Language models like BERT have shown impressive capabilities for natural language tasks, inspiring their application to model protein sequences. These protein language models (PLMs) treat amino acid sequences analogous to sentences, with patterns reflecting structural and functional constraints. For instance, local amino acid order directly specifies secondary structure, while distal coevolving residues often contact in 3D space. Local amino acid order patterns such as helices and strands dictate the resulting local protein secondary structure. Meanwhile, amino acids distant in sequence but close spatially often coevolve by compensatory mutations to maintain structural integrity.</p> <p>PLMs are commonly pretrained via objectives like masked language modeling (MLM), which trains the model to predict randomly masked tokens from context. Despite simple self-supervision, large PLMs exhibit emergent understanding of language semantics. This motivated hypothesis that pretraining PLMs on protein corpora may similarly elicit learning of sequence-structure mappings, given patterns in sequences encode structural properties. Because patterns in protein sequences inherently reflect structural constraints, pretraining large PLMs to model sequences may enable learning about the sequence-structure relationship. This could allow PLMs to infer 3D structural properties purely from sequence, despite no explicit supervision.</p> <p>Critically, insights from statistical mechanics and coevolutionary sequence analysis established that proteins’ 3D shapes constrain viable amino acid substitutions over evolution. Thus structural information becomes embedded in sequence statistics. Early bioinformatics methods leveraged this to predict aspects like secondary structure or residue contacts.</p> <p>Machine learning advanced contact and structure prediction. Initial methods employed convolutional or recurrent architectures. Attention mechanisms then enabled modeling long-range dependencies, improving contact prediction. Recently, large multitask networks integrating both sequence and structure data have achieved highly accurate structure prediction.</p> <p>In parallel, natural language processing advanced via scaling model size and data. Self-attention-based transformers pretrained on text via MLM exhibit strong few-shot learning abilities. ProtBert first adapted BERT to proteins, showing MLM pretraining improved generalization. Subsequent PLMs including ProtT5 and ESM-1b demonstrated increasing capability to capture protein properties given sufficient scale.</p> <p>Scaling up MLM pretraining on large, diverse protein corpora may thus enable PLMs to implicitly learn mappings between sequence statistics and 3D conformations. This contrasts classic template-based or homology modeling, which compares query sequences against evolutionary relatives with known structures. PLMs can potentially associate sequences across fold space by identifying commonalities in sequence patterns induced by shared structural constraints. Realizing this possibility motivates recent research on large protein LM scaling.</p> <p>Overall, the convergence of statistical mechanics, coevolutionary theory, machine learning, and natural language processing over decades has laid the foundation for protein language models to potentially unlock new knowledge and capabilities as models scale. ESM-2 represents the latest innovation in this trajectory.</p> <h2 id="protein-language-models">Protein Language Models</h2> <p>Protein language models provide a new paradigm for learning representations integrating protein sequences’ structural and functional signatures. The core philosophy behind PLMs recognizes that sequences inherently encode signatures of 3D structures and biochemical activities into the order and variation of amino acids. This principle underpins advances in template modeling, physics-based simulations, and machine learned structure prediction.</p> <p>Despite straightforward pretraining objectives like MLM, large LMs demonstrate impressive zero-shot emergence of capabilities requiring understanding language semantics. For instance, models trained simply to fill masked words can exhibit nuanced skills like translation, summarization, and reasoning. Analogously, PLMs pretrained to predict masked amino acids from surrounding context may induce generalizable learning of sequence-structure relationships by internalizing meaningful sequence patterns.</p> <p>By pretraining on large, evolutionarily diverse sequence corpora, PLMs can potentially learn structural knowledge exceeding what is extractable from single protein families. PLMs complement traditional comparative modeling, which analyzes sequence covariation within groups of homologous proteins related by evolution. Modeling sequences collectively across protein space facilitates learning about fundamental physico-chemical constraints between sequences and structures.</p> <p>Recent PLMs exhibit promising capabilities for sequence-based structure prediction. For instance, models with over 10 billion parameters combined with differentiable folding heads can generate atomistic structural models from sequence alone. Some PLMs have achieved major speedups compared to previous methods while maintaining accuracy. Expanding protein sequence databases via metagenomic sequencing presents exciting opportunities for enlarging the scope of structures PLMs can effectively model.</p> <p>Overall, the ability of large neural language models to exhibit emergent capabilities has inspired approaches scaling up pretraining on protein sequences. ESM-2 substantiates the potential for this strategy to yield models encoding rich structural knowledge within sequence representations. Ongoing growth in model scale and available sequences promises continued development in protein language modeling.</p> <h2 id="emergence-of-structural-information-in-esm-2">Emergence of Structural Information in ESM-2</h2> <p>The ESM-2 model demonstrates substantial improvements in pretraining scale enables progressive emergence of protein structural properties in language model representations. Both conceptual insights into sequence-structure mappings and empirical experience with language models contributed to ESM-2’s development.</p> <table> <thead> <tr> <th style="text-align: center"><img src="https://imgur.com/0YIhOEp.png" width="75%" height="75%"/></th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><em>ESM-2 Model</em></td> </tr> </tbody> </table> <p>Theoretical principles established proteins’ 3D conformations constrain viable amino acid substitutions over evolution, embedding structural signatures within sequence statistics. This motivated pioneering bioinformatics approaches to leverage statistics for structure and function insight. Meanwhile, large neural language models, despite simple objectives like masked LM pretraining, exhibited surprising emergent capabilities requiring understanding language semantics.</p> <p>ESM-2 synthesizes these perspectives. It adopts a 24-layer transformer encoder architecture, with modifications including rotary position embeddings to better model long protein sequences. ESM-2 is pretrained on 65 million unique UniRef50 sequences using masked LM, enabling the model to learn sequence patterns and their structural implications.</p> <p>Analyses reveal consistent improvements in structural knowledge as ESM-2 scaling progresses from 8 million to 15 billion parameters. Both low-resolution and atomic-level structure quality metrics substantially increase with model size. Perplexity measurements also improve, confirming scaling enhances sequence representations. Critically, proteins exhibiting perplexity gains also show structure prediction gains, evidencing intimate connections between sequence modeling and structure learning. As ESM-2 scaling progressed from 8 million to 15 billion parameters, long-range contact precision increased substantially from 0.16 to 0.54 (238% relative gain). This metric measuring how accurately residue contacts can be extracted from self-attention maps improved steadily with model size. Similarly, CASP14 TM-score rose from 0.37 to 0.55 (49% relative gain), indicating atomic-level structure quality improved with scale. Perplexity declined from 10.45 to 6.37, confirming language modeling of sequences enhanced across scaling. Critically, proteins with large perplexity gains also showed large contact prediction gains (NDCG = 0.87), evidencing tight coupling between sequence modeling and structure prediction quality.</p> <p>Together, these results substantiate the hypothesis that massive self-supervision on diverse protein sequences can elicit emergence of structural knowledge within sequence representations. This supports the promise of large language model pretraining for deducing 3D conformations from sequence statistics by integrating co-evolutionary signals across protein space.</p> <p>The innovations of ESM-2 provide a foundation to further scale model capacity, sequence data, and compute power. Ongoing growth along these dimensions will offer insight into the limits of structural knowledge extractable purely from unsupervised sequence modeling.</p> <h2 id="esmfold-architecture">ESMFold Architecture</h2> <p>Building on insights from ESM-2, the ESMFold model enables fast, end-to-end protein structure prediction directly from sequence. ESMFold integrates the pretrained ESM-2 language model with a lightweight prediction head module. This simple yet powerful approach eliminates the need for homology search or complex multitask architectures.</p> <p>ESM-2 provides a strong foundation, having accumulated rich structural knowledge from its masked LM pretraining. Its representations are fed into ESMFold’s prediction module, which applies alternating layers of self- and pairwise attention to refine coordinate predictions. A triangular update scheme helps capture long-range dependencies, while maintaining efficiency.</p> <p>This overall architecture optimizes prediction speed without sacrificing accuracy. By exploiting the compressed evolutionary signal within ESM-2, costly alignment procedures are avoided. The prediction module also eliminates much complexity from state-of-the-art pipelines. Together, these savings allow up to 60x faster inference than existing methods while achieving competitive accuracy.</p> <p>ESM-2’s training is critical to ESMFold’s performance. Masked LM over 65+ million diverse sequences enables the language model to learn generalizable associations between sequence patterns and structural constraints. This rich pretraining endows ESMFold with single sequence prediction abilities exceeding alignment-dependent alternatives when alignments are ablated.</p> <p>Furthermore, ESM-2 perplexity predicts ESMFold accuracy, since perplexity improvements imply enhanced sequence representations that provide ESMFold superior structural insight. Overall, ESMFold’s design elegantly combines the power of scale with architectural simplicity to advance protein structure prediction.</p> <table> <thead> <tr> <th style="text-align: center"><img src="https://imgur.com/UKcvZas.png" width="75%" height="75%"/></th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><em>ESM-2 Model Perplexity</em></td> </tr> </tbody> </table> <p>The innovations of ESMFold highlight the potential for pretrained language models to accelerate structure prediction. Its speed and accuracy could expand high-quality prediction to large databases like metagenomic resources. Future work on integrating coevolutionary signals and physics-based refinement may further improve accuracy.</p> <h2 id="conclusions">Conclusions</h2> <p>In conclusion, ESM-2 and ESMFold demonstrate transformative potential for large language model pretraining in protein structure prediction. By pretraining on massive and diverse protein sequence corpora, representations emerge that encapsulate surprisingly detailed structural knowledge purely from sequence signals. This knowledge enables fast yet accurate end-to-end structure prediction from sequence alone.</p> <p>ESM-2 establishes clear trends of improving structural understanding with greater model scale and data diversity. Ongoing expansion of sequence databases via metagenomics promises continued gains from larger pretraining resources. In tandem, architectural advances may further enhance sequence representations.</p> <p>Looking forward, large language models like ESM-2 offer an exciting path to democratize structure prediction by distilling vast structural knowledge from sequences alone. Realizing this vision could accelerate discovery of new protein structures, functions, and mechanisms across biology. More broadly, scaling self-supervision may prove a general paradigm for eliciting emergence of complex reasoning abilities in artificial intelligence systems.</p> <p>The insights gained from ESM-2 and ESMFold underscore both the progress enabled by scaled language model pretraining as well as the vast room for continued innovation. Expanding compute power, data, and model architectures promises to further advance protein language modeling and structure prediction into the future.</p> <h2 id="references">References</h2> <ol> <li><a href="https://www.science.org/doi/10.1126/science.ade2574">Evolutionary-scale prediction of atomic-level protein structure with a language model</a></li> <li><a href="https://proceedings.mlr.press/v139/rao21a.html">MSA Transformer</a></li> <li><a href="https://www.nature.com/articles/s41586-021-03819-2/">Highly accurate protein structure prediction with AlphaFold</a></li> <li><a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a></li> <li><a href="https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe">Language Models are Unsupervised Multitask Learners</a></li> <li><a href="https://www.pnas.org/doi/10.1073/pnas.1111471108">Direct-coupling analysis of residue coevolution captures native contacts across many protein families</a></li> </ol>]]></content><author><name>Amitesh Badkul</name></author><category term="literature-review"/><category term="ml"/><category term="dl"/><category term="literature"/><summary type="html"><![CDATA[ESM2 model explained]]></summary></entry><entry><title type="html">Dynamics and Properties of Cdh23EC1</title><link href="https://amiteshbadkul.github.io/blog/2023/md-analysis/" rel="alternate" type="text/html" title="Dynamics and Properties of Cdh23EC1"/><published>2023-04-20T15:59:00+00:00</published><updated>2023-04-20T15:59:00+00:00</updated><id>https://amiteshbadkul.github.io/blog/2023/md-analysis</id><content type="html" xml:base="https://amiteshbadkul.github.io/blog/2023/md-analysis/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Proteins are central to molecular biology due to their complexity and dynamic nature. They perform a range of functions crucial for the sustenance of life. Molecular Dynamics (MD) simulations provide a detailed perspective on the behavior of proteins at the atomic level, allowing researchers to study their dynamics and interactions over time. In this exploration, we’ll focus on the Cdh23EC1 protein, delving into its unique properties and characteristics. This simulation provides insights into the dynamic behaviors and interactions essential to Cdh23EC1’s structure and function. Cdh23EC1 is an extracellular domain of the cadherin protein Cdh23, which plays a critical role in hearing and balance—understanding how Cdh23EC1 moves and interacts is vital for elucidating its mechanosensory function. The MD simulation data used in this analysis was provided by <a href="https://research.cbc.osu.edu/sotomayor.8/teaching/">Marcos Sotomayor</a>, who performed simulations using the software NAMD and visualization using the VMD software. The simulation is described extensively <a href="https://research.cbc.osu.edu/sotomayor.8/wp-content/uploads/2014/10/namd-tutorial.pdf">here</a>, and you can check it out for more details.</p> <h3 id="the-significance-of-md-simulation"><strong>The Significance of MD Simulation</strong></h3> <p>Molecular Dynamics (MD) simulations have become an indispensable tool in computational biology. MD simulations deliver insights into proteins’ temporal and spatial behaviors by modeling atomic movements within a system. This level of detail surpasses many traditional experimental techniques. With every atom’s position documented at each time step, the resulting datasets are expansive and rich in information. The sheer detail captured in these simulations is awe-inspiring, with every atom’s position recorded at each time step, leading to vast datasets that encapsulate the protein’s life story.</p> <p>Such comprehensive data comes with its challenges. It demands rigorous analysis to extract meaningful insights. The vastness of this data can be overwhelming, but therein lies its beauty. We can unravel the mysteries of protein behavior by dissecting and analyzing this data, as we will do in this post. From understanding structural stability and flexibility to pinpointing crucial interactions that dictate function, the analyses derived from MD simulations are invaluable. MD simulations, thus, enable us to perceive proteins not merely as static structures but as dynamic entities exhibiting complex behaviors in a simulated environment.</p> <table> <thead> <tr> <th style="text-align: center"><img src="https://upload.wikimedia.org/wikipedia/commons/4/49/Protein_CDH23_PDB_2KBR.png" width="75%" height="75%"/></th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><em>The intricate Cadherin-23 Protein</em></td> </tr> </tbody> </table> <p>To run a simple MD Simulation with only water molecules checkout <a href="https://amiteshbadkul.github.io/blog/2022/md-simulation/">here</a>.</p> <h3 id="types-of-analysis"><strong>Types of Analysis</strong></h3> <p>In molecular dynamics (MD) simulations, data analysis is paramount. The sheer volume of data generated can be overwhelming. Still, by breaking it down into specific types of analysis, we can extract meaningful insights about the behavior of the protein in question. For our study on Cdh23EC1, we conducted the following analyses:</p> <ol> <li> <p>Mean Square Displacement (MSD) and Diffusion Coefficient Quantifies the average motion of particles over time. The MSD measures the average squared displacement of atoms over time intervals, while the diffusion coefficient (derived from MSD) gauges how fast these particles diverge from their initial positions.</p> </li> <li> <p>Root Mean Square Fluctuations (RMSF) RMSF provides insights into flexibility across different protein regions. Regions with high RMSF values hint at protein domains that are functionally significant or areas of interaction with other molecules.</p> </li> <li> <p>Hydrogen Bonds Over Time Hydrogen bonds are crucial for protein stability and structure. Monitoring their number over time can offer insights into the protein’s stability and possible conformational shifts.</p> </li> <li> <p>Salt Bridge Occupancies Salt bridges are electrostatic interactions between oppositely charged residues. Tracking consistent interactions can highlight vital stabilizing regions in the protein.</p> </li> <li> <p>Position Autocorrelation Function (PACF) Evaluates atomic position consistency over time. A high autocorrelation suggests stable regions within the protein.</p> </li> <li> <p>Radius of Gyration This metric gauges the protein’s compactness. Fluctuations in the radius can signal conformational shifts, such as protein folding or unfolding.</p> </li> <li> <p>Contact Map Generation A contact map visually represents protein interactions, revealing which residues are in close proximity and helping identify potential interaction sites or protein domains.</p> </li> </ol> <p>By conducting these analyses, we aim to piece together a comprehensive understanding of Cdh23EC1’s dynamics and behavior during the simulation. Each method offers a unique lens, and when combined, they provide a holistic view of the protein’s properties and interactions.</p> <h3 id="msd-and-diffusion-coefficient-assessing-protein-mobility"><strong>MSD and Diffusion Coefficient: Assessing Protein Mobility</strong></h3> <p><strong>Importance:</strong><br/> Mean Square Displacement (MSD) is a critical metric that measures the average squared distance traveled by particles, providing a quantifiable view into the mobility of atoms in a system over time. The Diffusion Coefficient, derived from MSD, represents the rate at which molecules disperse from their initial positions. This coefficient is instrumental in characterizing transport behaviors in molecular systems.</p> <p><strong>Code:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Select atoms
</span><span class="n">all_atoms</span> <span class="o">=</span> <span class="n">u</span><span class="p">.</span><span class="nf">select_atoms</span><span class="p">(</span><span class="sh">"</span><span class="s">all</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Initialize variables for MSD calculation
</span><span class="n">n_frames</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">u</span><span class="p">.</span><span class="n">trajectory</span><span class="p">)</span>
<span class="n">msd</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">n_frames</span><span class="p">)</span>

<span class="c1"># Loop over trajectory to calculate MSD
</span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ts</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">u</span><span class="p">.</span><span class="n">trajectory</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">initial_positions</span> <span class="o">=</span> <span class="n">all_atoms</span><span class="p">.</span><span class="n">positions</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
    <span class="n">msd</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">square</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">all_atoms</span><span class="p">.</span><span class="n">positions</span> <span class="o">-</span> <span class="n">initial_positions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)))</span>

<span class="c1"># Calculate diffusion coefficient (D = MSD / 6t)
</span><span class="n">delta_time</span> <span class="o">=</span> <span class="n">u</span><span class="p">.</span><span class="n">trajectory</span><span class="p">.</span><span class="n">dt</span>  <span class="c1"># Time step between frames in ps (pico-seconds)
</span><span class="n">time</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">n_frames</span><span class="p">)</span> <span class="o">*</span> <span class="n">delta_time</span>
<span class="n">diffusion_coefficient</span> <span class="o">=</span> <span class="n">msd</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="mi">6</span> <span class="o">*</span> <span class="n">time</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="c1"># Plot MSD vs time
</span><span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">msd</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Time (ps)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">MSD (Å²)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Diffusion Coefficient:</span><span class="sh">"</span><span class="p">,</span> <span class="n">diffusion_coefficient</span><span class="p">,</span> <span class="sh">"</span><span class="s">Å²/ps</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <table> <thead> <tr> <th style="text-align: center"><img src="https://imgur.com/zeBWuPa.png" width="75%" height="75%"/></th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><em>The Mean Square Displacement graph along time, indicating continous motion in the system</em></td> </tr> </tbody> </table> <p><strong>Analysis:</strong><br/> For Cdh23EC1, a systematic observation of atomic movements was made. The provided MSD values began at zero and displayed a consistent upward trend, emphasizing the continual motion of atoms in the protein throughout the simulation. With the MSD data at hand, the calculated Diffusion Coefficient stood at (1.767 \, \text{Å}^2/\text{ps}). This value indicates a moderate diffusion rate, suggesting that Cdh23EC1 is neither entirely static nor exceptionally mobile in its given environment.</p> <p><strong>Conclusions:</strong><br/> The data-driven insights from the MSD and Diffusion Coefficient values underscore Cdh23EC1’s dynamic behavior in the simulated conditions. Its moderate diffusion rate implies a balance between the protein’s interactions: it’s not strictly bound in place but doesn’t disperse rapidly. This diffusive character could be attributed to interactions with surrounding solvent molecules, inherent protein structural dynamics, or, likely, an interplay of both factors. Such insights are crucial in understanding how the protein behaves and interacts within its milieu.</p> <h3 id="root-mean-square-fluctuations-rmsf-decoding-residue-dynamics"><strong>Root Mean Square Fluctuations (RMSF): Decoding Residue Dynamics</strong></h3> <p><strong>Importance:</strong><br/> RMSF is a pivotal metric in molecular dynamics, spotlighting the variability in movement across residues in a protein. By interpreting these fluctuations, we can pinpoint regions exuding stability versus those with enhanced flexibility. Understanding such dynamism is essential, as it can demystify functionally significant zones in a protein, potentially involved in various molecular interactions or structural maintenance.</p> <p><strong>Code:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">MDAnalysis.analysis</span> <span class="kn">import</span> <span class="n">rms</span>

<span class="c1"># Select C-alpha atoms
</span><span class="n">calphas</span> <span class="o">=</span> <span class="n">u</span><span class="p">.</span><span class="nf">select_atoms</span><span class="p">(</span><span class="sh">"</span><span class="s">name CA</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Calculate RMSF
</span><span class="n">rmsfer</span> <span class="o">=</span> <span class="n">rms</span><span class="p">.</span><span class="nc">RMSF</span><span class="p">(</span><span class="n">calphas</span><span class="p">).</span><span class="nf">run</span><span class="p">()</span>

<span class="c1"># Plot RMSF
</span><span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rmsfer</span><span class="p">.</span><span class="n">rmsf</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Residue Number</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">RMSF (Å)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <table> <thead> <tr> <th style="text-align: center"><img src="https://imgur.com/kTS6IyJ.png" width="75%" height="75%"/></th> </tr> </thead> <tbody> <tr> <td style="text-align: center">*RMSF showing motions for different amino acids *</td> </tr> </tbody> </table> <p><strong>Analysis:</strong><br/> For Cdh23EC1, the RMSF values presented a mixed bag. The values ranged from approximately (1.2 \, \text{Å}) to a more pronounced fluctuation of over (2.4 \, \text{Å}). This variance suggests that while some protein parts remain relatively stable, other regions exhibit significant motion. For instance, residues that demonstrated lower RMSF values (around (1.2-1.4 \, \text{Å})) are suggestive of a stable core, vital for preserving the protein’s structural essence. Conversely, regions with RMSF values exceeding (2.2 \, \text{Å}) indicate pronounced flexibility. Such flexibility is typically associated with external protein loops, which are more dynamic, facilitating their interactions with the external milieu. The RMSF analysis also revealed distinct patterns of residue flexibility throughout the protein. Residues such as MET1, GLN2, and VAL3 showed moderate fluctuations, indicating a balanced behavior. In contrast, residues like ARG5, LEU6, and ASP37 exhibited higher fluctuations, suggesting they might be located in more flexible regions like loops or terminal. Conversely, residues with lower RMSF values, such as PHE8, THR10, and HSD12, likely belong to the protein’s more stable core or are part of structured domains.</p> <p><strong>Conclusions:</strong><br/> Cdh23EC1’s RMSF profile portrays a protein that harmoniously blends structural rigidity with molecular adaptability. The stable core regions are juxtaposed against flexible external loops, each playing a distinct role. While the core provides structural integrity, the flexible zones might be central to Cdh23EC1’s interactive capabilities, whether that involves binding, environmental sensing, or other molecular interactions. This dynamic equilibrium underscores the protein’s multifaceted nature, hinting at its complex functional roles during the simulation.</p> <h3 id="hydrogen-bonds-crucial-molecular-interactions"><strong>Hydrogen Bonds: Crucial Molecular Interactions</strong></h3> <p><strong>Importance:</strong><br/> Hydrogen bonds, while individually weak, cumulatively exert a profound influence on protein behavior. These interactions are instrumental in stabilizing protein structures, guiding protein folding pathways, and facilitating dynamic processes such as enzyme catalysis and substrate binding.</p> <p><strong>Code:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Perform Hydrogen Bond Analysis
</span><span class="n">h</span> <span class="o">=</span> <span class="n">hydrogenbonds</span><span class="p">.</span><span class="nc">HydrogenBondAnalysis</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="sh">'</span><span class="s">protein</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">protein</span><span class="sh">'</span><span class="p">)</span>
<span class="n">h</span><span class="p">.</span><span class="nf">run</span><span class="p">()</span>

<span class="c1"># The results are stored as a NumPy array
</span><span class="n">hbond_data</span> <span class="o">=</span> <span class="n">h</span><span class="p">.</span><span class="n">hbonds</span>

<span class="c1"># Count the number of hydrogen bonds at each time frame
</span><span class="n">unique_times</span><span class="p">,</span> <span class="n">hbond_counts</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">unique</span><span class="p">(</span><span class="n">hbond_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">return_counts</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Plotting
</span><span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">unique_times</span><span class="p">,</span> <span class="n">hbond_counts</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Time (ps)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Number of Hydrogen Bonds</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Hydrogen Bonds Over Time</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <table> <thead> <tr> <th style="text-align: center"><img src="https://imgur.com/K0vOfS2.png" width="75%" height="75%"/></th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><em>Hydrogen Bonding interactions and how are they are affected over time</em></td> </tr> </tbody> </table> <p><strong>Analysis:</strong><br/> A comprehensive analysis of Cdh23EC1’s hydrogen bonding patterns showcased a fluctuating profile. The count of hydrogen bonds varied as the simulation progressed, indicating a protein that constantly reshapes its hydrogen bond network, adapting to environmental changes and interactions. Specifically, the data showed hydrogen bond counts ranging between 340 and 384 across different time points. From the data, specific residues like ARG5 showcased close interactions with residues like ASP39 and ASP37. Given the opposing charges of ARG (positively charged guanidinium group) and ASP (negatively charged carboxylate group), it’s plausible that these residues engage in hydrogen bonding. Such interactions can not only stabilize the protein’s structure but also play roles in dynamic processes like enzyme catalysis or mediating allosteric effects.</p> <p><strong>Conclusions:</strong><br/> The dynamic behavior of hydrogen bonds in Cdh23EC1 reflects the protein’s adaptability in its surroundings. These bonds’ continual formation and dissolution could be attributed to protein conformational shifts, interplay with surrounding solvent molecules, or potential interactions with other molecular entities. By pinpointing the locations and duration of these hydrogen bonds, we can derive valuable insights into pivotal regions of the protein, potentially crucial for its primary functions and structural integrity.</p> <h3 id="salt-bridge-occupancies-electrostatic-interactions-at-play"><strong>Salt Bridge Occupancies: Electrostatic Interactions at Play</strong></h3> <p><strong>Importance:</strong><br/> Salt bridges are pivotal electrostatic interactions between oppositely charged residues in a protein. They play a central role in ensuring protein stability, influencing folding patterns, and sometimes even dictating function. By studying the formation and consistency of these bridges, we can gain insights into the structural and functional intricacies of the protein.</p> <p><strong>Code:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Select positively and negatively charged residues
</span><span class="n">pos_residues</span> <span class="o">=</span> <span class="n">u</span><span class="p">.</span><span class="nf">select_atoms</span><span class="p">(</span><span class="sh">"</span><span class="s">resname ARG LYS</span><span class="sh">"</span><span class="p">)</span>
<span class="n">neg_residues</span> <span class="o">=</span> <span class="n">u</span><span class="p">.</span><span class="nf">select_atoms</span><span class="p">(</span><span class="sh">"</span><span class="s">resname ASP GLU</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Initialize dictionary to store salt bridge occupancy data
</span><span class="n">salt_bridge_occupancy</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="c1"># Total number of frames in the trajectory
</span><span class="n">total_frames</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">u</span><span class="p">.</span><span class="n">trajectory</span><span class="p">)</span>

<span class="c1"># Loop through each frame of the trajectory
</span><span class="k">for</span> <span class="n">ts</span> <span class="ow">in</span> <span class="n">u</span><span class="p">.</span><span class="n">trajectory</span><span class="p">:</span>
    <span class="c1"># Calculate the distance between each pair of positive and negative residues
</span>    <span class="n">distance_matrix</span> <span class="o">=</span> <span class="n">mda</span><span class="p">.</span><span class="n">lib</span><span class="p">.</span><span class="n">distances</span><span class="p">.</span><span class="nf">distance_array</span><span class="p">(</span><span class="n">pos_residues</span><span class="p">.</span><span class="n">positions</span><span class="p">,</span> <span class="n">neg_residues</span><span class="p">.</span><span class="n">positions</span><span class="p">)</span>

    <span class="c1"># Identify pairs where distance &lt; 4 Å
</span>    <span class="n">salt_bridges</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">distance_matrix</span> <span class="o">&lt;</span> <span class="mf">4.0</span><span class="p">)</span>

    <span class="c1"># Record residue pairs forming salt bridges in this frame
</span>    <span class="k">for</span> <span class="n">pos_idx</span><span class="p">,</span> <span class="n">neg_idx</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="o">*</span><span class="n">salt_bridges</span><span class="p">):</span>
        <span class="n">pos_residue</span> <span class="o">=</span> <span class="n">pos_residues</span><span class="p">[</span><span class="n">pos_idx</span><span class="p">].</span><span class="n">resid</span>
        <span class="n">neg_residue</span> <span class="o">=</span> <span class="n">neg_residues</span><span class="p">[</span><span class="n">neg_idx</span><span class="p">].</span><span class="n">resid</span>
        <span class="n">salt_bridge_occupancy</span><span class="p">[(</span><span class="n">pos_residue</span><span class="p">,</span> <span class="n">neg_residue</span><span class="p">)]</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="c1"># Calculate occupancy as the fraction of the total frames
</span><span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">salt_bridge_occupancy</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
    <span class="n">salt_bridge_occupancy</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span> <span class="o">/</span> <span class="n">total_frames</span>

<span class="c1"># Print salt bridge occupancies
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Salt Bridge Occupancies:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">for </span><span class="p">(</span><span class="n">pos_residue</span><span class="p">,</span> <span class="n">neg_residue</span><span class="p">),</span> <span class="n">occupancy</span> <span class="ow">in</span> <span class="n">salt_bridge_occupancy</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Salt bridge between residue </span><span class="si">{</span><span class="n">pos_residue</span><span class="si">}</span><span class="s"> and </span><span class="si">{</span><span class="n">neg_residue</span><span class="si">}</span><span class="s"> has an occupancy of </span><span class="si">{</span><span class="n">occupancy</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Salt Bridge Occupancies:
Salt bridge between residue 5 and 39 has an occupancy of 48.40
Salt bridge between residue 5 and 37 has an occupancy of 6.40
Salt bridge between residue 5 and 87 has an occupancy of 1.40
Salt bridge between residue 36 and 37 has an occupancy of 49.70
Salt bridge between residue 54 and 50 has an occupancy of 28.60
Salt bridge between residue 54 and 51 has an occupancy of 32.40
Salt bridge between residue 73 and 22 has an occupancy of 17.00
Salt bridge between residue 73 and 72 has an occupancy of 29.20
Salt bridge between residue 73 and 74 has an occupancy of 30.50
Salt bridge between residue 73 and 102 has an occupancy of 0.40
Salt bridge between residue 76 and 72 has an occupancy of 6.10
Salt bridge between residue 76 and 74 has an occupancy of 6.00
Salt bridge between residue 76 and 78 has an occupancy of 22.00
Salt bridge between residue 76 and 51 has an occupancy of 20.80
Salt bridge between residue 94 and 15 has an occupancy of 20.70
Salt bridge between residue 94 and 82 has an occupancy of 5.50
Salt bridge between residue 95 and 82 has an occupancy of 33.70
</code></pre></div></div> <table> <thead> <tr> <th style="text-align: center"><img src="https://imgur.com/qj4XjqZ.png" width="75%" height="75%"/></th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><em>Salt Bridge heatmap</em></td> </tr> </tbody> </table> <p><strong>Analysis:</strong><br/> In our in-depth analysis of Cdh23EC1, several key salt bridge interactions emerged:</p> <ol> <li> <p><strong>ARG5 and ASP39</strong>: With a high occupancy of 48.40%, this interaction is particularly stable throughout the simulation. The positive guanidinium group of ARG (residue 5) and the negatively charged carboxylate group of ASP (residue 39) create a strong electrostatic attraction. Given its consistent presence, this salt bridge likely plays a pivotal role in maintaining the structural integrity of Cdh23EC1.</p> </li> <li> <p><strong>ARG36 and ASP37</strong>: Another prominent salt bridge with an occupancy of 49.70%. The proximity of these residues in the sequence and their high interaction frequency indicate a crucial intramolecular interaction, possibly stabilizing a specific region of the protein.</p> </li> <li> <p><strong>LYS76 and ASP72</strong>: This interaction, though not as predominant as the previous ones, showcases an occupancy of 6.10%. The positive amino group of LYS (residue 76) likely interacts with the negatively charged carboxylate group of ASP (residue 72), further adding to the protein’s electrostatic network.</p> </li> <li> <p><strong>Other noteworthy interactions</strong>:</p> <ul> <li>ARG54’s interactions with both GLU50 (28.60%) and GLU51 (32.40%)</li> <li>ARG73’s interactions with GLU22 (17.00%), ASP72 (29.20%), and GLU74 (30.50%)</li> <li>LYS94’s interactions with ASP15 (20.70%) and GLU82 (5.50%)</li> <li>LYS95’s interaction with GLU82, showcasing a significant occupancy of 33.70%</li> </ul> </li> </ol> <p>These salt bridges, collectively, weave a dense tapestry of electrostatic interactions throughout Cdh23EC1. Their presence and frequency hint at regions of the protein that might be pivotal for its function, stability, or interaction with other molecules.</p> <p>In conclusion, the electrostatic interplay observed in Cdh23EC1 underscores its dynamic nature and the intricate balance it maintains between stability and adaptability. Recognizing these interactions and their implications can significantly enhance our understanding of the protein’s behavior and functionality.</p> <p><strong>Conclusions:</strong><br/> The consistent presence of certain salt bridges in Cdh23EC1 underscores their importance in the protein’s structural and functional landscape. These electrostatic interactions can serve as stabilizing forces, guide the protein folding process, or even modulate the protein’s behavior in response to external stimuli. By understanding where these salt bridges are situated and their relative stability, we can deduce potential regions of the protein that are vital for its function, stability, or interaction with other molecules.</p> <h3 id="position-autocorrelation-function-pacf-tracing-residual-movements-over-time"><strong>Position Autocorrelation Function (PACF): Tracing Residual Movements Over Time</strong></h3> <p><strong>Importance:</strong><br/> Proteins are dynamic entities, with residues exhibiting specific movement patterns over time. The Position Autocorrelation Function (PACF) is a tool that captures these patterns, allowing researchers to identify periodic or repetitive motions in a protein’s structure. Such movements can be crucial determinants of various protein functions, ranging from enzyme activities to specific protein-ligand or protein-protein interactions.</p> <p><strong>Code:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Select C-alpha atoms
</span><span class="n">calphas</span> <span class="o">=</span> <span class="n">u</span><span class="p">.</span><span class="nf">select_atoms</span><span class="p">(</span><span class="sh">"</span><span class="s">name CA</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Number of frames and atoms
</span><span class="n">n_frames</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">u</span><span class="p">.</span><span class="n">trajectory</span><span class="p">)</span>
<span class="n">n_atoms</span> <span class="o">=</span> <span class="n">calphas</span><span class="p">.</span><span class="n">n_atoms</span>

<span class="c1"># Initialize array to store positions and PACF
</span><span class="n">positions</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">n_frames</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">pacf</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">n_frames</span><span class="p">)</span>

<span class="c1"># Loop through trajectory to get positions
</span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ts</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">u</span><span class="p">.</span><span class="n">trajectory</span><span class="p">):</span>
    <span class="n">positions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">calphas</span><span class="p">.</span><span class="n">positions</span>

<span class="c1"># Calculate PACF
</span><span class="k">for</span> <span class="n">dt</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_frames</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_frames</span> <span class="o">-</span> <span class="n">dt</span><span class="p">):</span>
        <span class="n">pacf</span><span class="p">[</span><span class="n">dt</span><span class="p">]</span> <span class="o">+=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">positions</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="n">positions</span><span class="p">[</span><span class="n">t</span> <span class="o">+</span> <span class="n">dt</span><span class="p">])</span>
    <span class="n">pacf</span><span class="p">[</span><span class="n">dt</span><span class="p">]</span> <span class="o">/=</span> <span class="p">(</span><span class="n">n_atoms</span> <span class="o">*</span> <span class="p">(</span><span class="n">n_frames</span> <span class="o">-</span> <span class="n">dt</span><span class="p">))</span>

<span class="c1"># Plot PACF
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">n_frames</span><span class="p">)</span> <span class="o">*</span> <span class="n">u</span><span class="p">.</span><span class="n">trajectory</span><span class="p">.</span><span class="n">dt</span><span class="p">,</span> <span class="n">pacf</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Time (ps)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">PACF</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Position Autocorrelation Function for C-alpha atoms</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <table> <thead> <tr> <th style="text-align: center"><img src="https://imgur.com/y5Nj3BC.png" width="75%" height="75%"/></th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><em>PACF for the backbone of the proteins</em></td> </tr> </tbody> </table> <p><strong>Analysis:</strong><br/> In our analysis of Cdh23EC1’s PACF, we observed that specific residues, especially the C-alpha atoms, exhibit a higher tendency for periodic motions. But why focus on C-alpha atoms? The C-alpha atom is the central carbon atom in an amino acid’s backbone, representing the backbone’s position without the side chain’s intricacies. Analyzing the C-alpha atoms simplifies the computational complexity and provides a clearer picture of the protein’s backbone dynamics. Including all atoms might add noise to the data due to side chain motions, which can sometimes overshadow the more subtle, yet significant, movements of the protein backbone. From our data, it becomes evident that these periodic movements are not uniformly distributed across the protein. Certain regions show pronounced periodicity, suggesting areas that undergo consistent and repetitive motions. These might be hinge regions, allowing for larger domain movements, or they could be areas involved in active site dynamics, ligand binding, or other functional mechanisms.</p> <p><strong>Conclusions:</strong><br/> The PACF patterns observed in Cdh23EC1 indicate the presence of residues or regions undergoing periodic or repetitive motions. These motions can be critical for the protein’s overall function, aiding in substrate binding or allosteric regulation. Identifying and understanding these regions can provide valuable insights into their functional roles in the protein’s lifecycle and their potential impact on interactions with other biomolecules.</p> <h3 id="radius-of-gyration-assessing-protein-compactness"><strong>Radius of Gyration: Assessing Protein Compactness</strong></h3> <p><strong>Importance:</strong><br/> A protein’s atoms’ overall shape and spatial distribution play a pivotal role in its interactions and functions. The radius of gyration serves as a metric to evaluate this spatial distribution. It provides an understanding of how spread out the atoms of a protein are from its center of mass, thus shedding light on its compactness, conformation, and potential flexibility.</p> <p><strong>Code:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create an AtomGroup containing all atoms
</span><span class="n">all_atoms</span> <span class="o">=</span> <span class="n">u</span><span class="p">.</span><span class="nf">select_atoms</span><span class="p">(</span><span class="sh">"</span><span class="s">all</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Initialize an empty list to store radius of gyration values
</span><span class="n">rg_values</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Iterate over the trajectory and calculate the radius of gyration at each frame
</span><span class="k">for</span> <span class="n">ts</span> <span class="ow">in</span> <span class="n">u</span><span class="p">.</span><span class="n">trajectory</span><span class="p">:</span>
    <span class="n">rg</span> <span class="o">=</span> <span class="n">all_atoms</span><span class="p">.</span><span class="nf">radius_of_gyration</span><span class="p">()</span>
    <span class="n">rg_values</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">rg</span><span class="p">)</span>

<span class="c1"># Plot the radius of gyration as a function of time
</span><span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rg_values</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Frame</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Radius of Gyration (Angstrom)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Radius of Gyration vs Time</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <table> <thead> <tr> <th style="text-align: center"><img src="https://imgur.com/nUahFd4.png" width="75%" height="75%"/></th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><em>Radius of Gyration of the protein indicating the stability of the fold</em></td> </tr> </tbody> </table> <p><strong>Analysis:</strong><br/> In our examination of Cdh23EC1, we noticed that the radius of gyration values oscillated between approximately 28.5 Å to 28.9 Å. This narrow range suggests that the protein maintains a relatively stable spatial configuration throughout the simulation period. Such stability in the radius of gyration indicates that the protein remains well-folded and does not undergo drastic conformational changes during the simulation.</p> <p><strong>Conclusions:</strong><br/> The consistent values of the radius of gyration for Cdh23EC1 are indicative of a protein that preserves its structural compactness over time. A stable radius of gyration suggests that the protein is likely to be in its native or functionally active conformation. By juxtaposing this data with other analyses, such as RMSF or hydrogen bond assessments, we can derive a comprehensive understanding of the regions in the protein that is both stable and flexible and how these attributes might contribute to the protein’s overall function.</p> <h3 id="contact-map-weaving-the-web-of-interactions"><strong>Contact Map: Weaving the Web of Interactions</strong></h3> <p><strong>Importance:</strong><br/> In the intricate 3D structure of proteins, residues often establish close spatial relationships, forming a network of interactions that underpin the molecule’s stability and function. These relationships can range from transient contacts, fleeting and situational, to persistent interactions that are foundational to the protein’s architecture. Contact maps serve as a powerful visualization tool, capturing these relationships and painting a detailed portrait of the protein’s interaction landscape.</p> <p><strong>Code:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">MDAnalysis</span> <span class="k">as</span> <span class="n">mda</span>
<span class="kn">from</span> <span class="n">MDAnalysis.analysis</span> <span class="kn">import</span> <span class="n">rms</span><span class="p">,</span> <span class="n">rdf</span>
<span class="kn">from</span> <span class="n">MDAnalysis.lib.distances</span> <span class="kn">import</span> <span class="n">distance_array</span>

<span class="c1"># Select the protein atoms
</span><span class="n">protein</span> <span class="o">=</span> <span class="n">u</span><span class="p">.</span><span class="nf">select_atoms</span><span class="p">(</span><span class="sh">"</span><span class="s">protein</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Initialize an empty list to store contact maps
</span><span class="n">contact_maps</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Loop through trajectory and calculate contact map for each frame
</span><span class="k">for</span> <span class="n">ts</span> <span class="ow">in</span> <span class="n">u</span><span class="p">.</span><span class="n">trajectory</span><span class="p">:</span>
    <span class="n">distances</span> <span class="o">=</span> <span class="nf">distance_array</span><span class="p">(</span><span class="n">protein</span><span class="p">.</span><span class="n">positions</span><span class="p">,</span> <span class="n">protein</span><span class="p">.</span><span class="n">positions</span><span class="p">)</span>
    <span class="n">contact_map</span> <span class="o">=</span> <span class="n">distances</span> <span class="o">&lt;</span> <span class="mf">6.0</span>  <span class="c1"># Contacts within 6 Angstroms
</span>    <span class="n">contact_maps</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">contact_map</span><span class="p">)</span>

<span class="c1"># Average the contact maps over all frames
</span><span class="n">average_contact_map</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">contact_maps</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Plotting the average contact map
</span><span class="n">sns</span><span class="p">.</span><span class="nf">heatmap</span><span class="p">(</span><span class="n">average_contact_map</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">coolwarm</span><span class="sh">'</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">square</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Average Contact Map</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Residue Index</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Residue Index</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <table> <thead> <tr> <th style="text-align: center"><img src="https://imgur.com/kjEnErN.png" width="75%" height="75%"/></th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><em>The Contact Map displaying the beautiful nature of proteins, their stabliity associated with their folds and interactions</em></td> </tr> </tbody> </table> <p><strong>Analysis:</strong><br/> Cdh23EC1’s contact map laid bare a dense network of interactions, each potentially playing a distinct role in the protein’s structural stability or function. Diving into Cdh23EC1’s contact map revealed a complex web of residue interactions. For instance, we see regular close contacts between residues like ARG5 and ASP39, reinforcing the potential electrostatic interactions we identified earlier in the salt bridge analysis. Such persistent interactions suggest regions of the protein that might be playing a pivotal role in its structural stability.</p> <p>Hydrophobic residues, nestled deep within the protein, are frequently in close proximity, hinting at van der Waals interactions or hydrophobic forces that serve as the backbone of the protein’s core. Additionally, the consistent proximity of certain residues suggests regions that might be engaged in hydrogen bonds. Notably, the contact map’s interactions support our earlier analysis where stable hydrogen bonds were identified.</p> <p><strong>Conclusions:</strong><br/> The myriad contacts within Cdh23EC1 provide a comprehensive view of its internal interactions. These contacts, whether driven by hydrophobic forces, electrostatic interactions, or simple van der Waals attractions, collectively define the protein’s shape, stability, and behavior. Understanding their nature and dynamics can offer deeper insights into Cdh23EC1’s functional domains and potential interaction sites.</p> <h2 id="discussion">Discussion</h2> <p>Through comprehensive analyses, we begin to understand the multifaceted nature of Cdh23EC1, characterized by its varied interactions and dynamic behaviors.</p> <h3 id="1-stability-vs-flexibility-the-delicate-balance">1. <strong>Stability vs Flexibility: The Delicate Balance</strong></h3> <p><strong>Hypothesis:</strong> Regions exhibiting higher RMSF values (indicative of flexibility) might have a decreased number of stabilizing interactions, such as hydrogen bonds or salt bridges, contributing to their increased mobility.</p> <p><strong>Deeper Dive:</strong><br/> Every protein strikes a delicate balance between maintaining its structure and allowing for flexibility. This flexibility can be vital for functions such as ligand binding or enzymatic activity. By comparing the RMSF plot (which showcases residue flexibility) with the distribution of hydrogen bonds and salt bridge occupancies, we can decipher how these molecular interactions influence regional dynamics. Stable core regions, with low RMSF values, should ideally be rich in these interactions, ensuring structural integrity. Conversely, external loops or regions with specific functional roles might have fewer of these interactions, allowing them the freedom to move and interact with their surroundings.</p> <h3 id="2-functional-domains-and-movement-the-dance-of-functionality">2. <strong>Functional Domains and Movement: The Dance of Functionality</strong></h3> <p><strong>Question:</strong> Do regions with pronounced RMSF values or periodic movements (as identified in the PACF analysis) correlate with known functional domains of Cdh23EC1?</p> <p><strong>Deeper Dive:</strong><br/> Proteins are not just static entities; they dance. This dance, especially in functional domains, can be crucial for molecular recognition, catalysis, or signaling. Overlaying our RMSF and PACF data with known functional domains from the literature or structural databases allows us to pinpoint regions of dynamic behavior that might be central to the protein’s function. Are these domains more flexible, allowing them to bind ligands or other proteins? Do they exhibit periodic movements, hinting at cyclic processes or allosteric mechanisms? By answering these questions, we connect the dots between structure, dynamics, and function.</p> <h3 id="3-compactness-and-interactions-the-network-within">3. <strong>Compactness and Interactions: The Network Within</strong></h3> <p><strong>Hypothesis:</strong> A stable core, as suggested by the consistent radius of gyration, is likely underpinned by a dense network of internal interactions.</p> <p><strong>Deeper Dive:</strong><br/> Like a bustling city, the interior of a protein is a hub of activity and interactions. These interactions, whether they’re between side chains just a few angstroms apart or residues on opposite ends of a secondary structure, collectively define the protein’s shape and behavior. By correlating our radius of gyration data (a measure of the protein’s spread) with our contact map (a visual representation of residue interactions), we can identify the key architectural elements that give Cdh23EC1 its unique form and function.</p> <h3 id="4-echoes-and-encounters-pacf-meets-the-contact-map">4. <strong>Echoes and Encounters: PACF Meets the Contact Map</strong></h3> <p><strong>Hypothesis:</strong> Regions exhibiting echoes of their past positions, as revealed in PACF, might be engaged in transient interactions, momentarily coming close to other residues.</p> <p><strong>Deeper Dive:</strong><br/> It’s fascinating to think that parts of a protein might “remember” past positions, exhibiting periodic movements. What drives these echoes? Could they be the result of transient interactions, momentary encounters with other residues that leave an imprint on their movement? By juxtaposing PACF data with the contact map, we can uncover patterns of transient interactions that might be driving these periodic motions, revealing the intricate dance of residues within Cdh23EC1.</p> <h2 id="conclusion-unraveling-the-dynamic-tapestry-of-cdh23ec1"><strong>Conclusion: Unraveling the Dynamic Tapestry of Cdh23EC1</strong></h2> <p>To effectively grasp a protein’s function, it’s essential to study both its structure and dynamic behavior. Our detailed examination of Cdh23EC1 using molecular dynamics (MD) simulations reveals a protein marked by intricate behaviors and interactions.</p> <p>From this study, Cdh23EC1 emerges as a complex entity, its movements and interactions shaped by its internal structure, surrounding environment, and inherent features. Our analyses provide a window into its behaviors, from flexibility patterns to transient interactions and periodic movements.</p> <p>Our approach underscores the significance of understanding molecular biology from multiple perspectives. A protein’s static structure is just one aspect; understanding its dynamics is equally crucial. This necessitates integrating various properties, from mean squared displacements to the radius of gyration, to gain a comprehensive view.</p> <p>Lastly, this investigation highlights the utility of MD simulations in contemporary research. They offer a dynamic perspective, enabling researchers to delve into the atomic intricacies of life. Proper interpretation of simulation results, backed by rigorous analysis and scientific curiosity, remains pivotal.</p> <p>In closing, Cdh23EC1 stands as a testament to the wonders of the molecular world. It’s a reminder that every protein, every molecule, has a story to tell. Our role, as scientists and curious minds, is to listen, to probe, and to understand these stories. The dance of atoms and residues, the transient interactions, the stable cores, and the dynamic exteriors, all come together to form the tapestry of life. And as we continue our explorations, armed with tools like MD simulations, we move one step closer to unraveling the mysteries that this tapestry holds.</p>]]></content><author><name></name></author><category term="molecular-dynamics"/><category term="md"/><category term="vmd"/><category term="namd"/><summary type="html"><![CDATA[Delving into the molecular dynamics of Cdh23EC1]]></summary></entry><entry><title type="html">Chest X-Ray Classification</title><link href="https://amiteshbadkul.github.io/blog/2022/cxr/" rel="alternate" type="text/html" title="Chest X-Ray Classification"/><published>2022-06-01T14:13:16+00:00</published><updated>2022-06-01T14:13:16+00:00</updated><id>https://amiteshbadkul.github.io/blog/2022/cxr</id><content type="html" xml:base="https://amiteshbadkul.github.io/blog/2022/cxr/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>To understand the importance of the features learnt while performing transfer learning in case of Deep Convolutional Neural Networks (DCNNs), I selected the problem of classification of Chest X-Rays into COVID-19 positive and negative. I intend to perform bit plane slicing and provide these bit planes separately as input to the DCNNs for transfer learning.</p> <h2 id="dataset">Dataset</h2> <p>The dataset used for the study can be obtained from <a href="https://www.kaggle.com/competitions/csc532">Kaggle</a>. The dataset consists of 676 images and 408 images for training and validation respectively belonging to both positive and negative class. The rest of the dataset, that is bit plane sliced images, are obtained manually as explained in the sections later.</p> <h2 id="preliminary-work">Preliminary Work</h2> <p>Initially, the performance of VGG16, InceptionV3, Inception-ResNet, DenseNet121, MobileNetV2, and ResNet101 is measured on the normal dataset to check which model performs the best. The general flow of pre-processing, training and evaluation is described as follows:</p> <ol> <li> <p>Firstly, we perform image augmentation. DCNNs frequently take a substantial amount of training data to attain high efficiency. Image augmentation is a method to increase the performance of DCNNs when creating a robust image classifier with very little training data. Image augmentation generates training images artificially using various processing methods or a mix of techniques, such as random rotation, shifts, shear, flips, etc. Earlier, the augmented images were saved along side the original dataset and provided to the DCNNs for training, however this caused memory constraints, therefore tensorflow introduced the concept of real-time data augmentation as the model trains, which is implemented in this study. This is done for both the training and the validation datasets.</p> </li> <li>Once pre-processing is complete, we begin to define the hyperparameters for training the model. The batch size refers to number of sample data points given to the model before it’s weights are updated. The batch size is defined as 32 because a higher batch size leads to degradation in the quality of the model as observed by its ability to generalize, whereas a lower batch size would increase the training time. Conceptually, we can think of the learning rate of our model as the step size. A learning rate of 0.0001 as it is the standard learning rate adapted by the community for transfer learning problems. Finally, we define the epochs as 50, which refers to number of times the model is trained on the entire dataset. <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Batch</span> <span class="n">Size</span>      <span class="mi">32</span>
<span class="n">Learning</span> <span class="n">Rate</span>   <span class="mf">0.0001</span>
<span class="n">Epochs</span>          <span class="mi">50</span>
</code></pre></div> </div> </li> <li>Lastly, the training of the model begins. Certain criterion are set for training, which include - early stopping, a condition where if a specified metric fails to improve for the duration of a certain number of epochs (defined as patience), the training stops. Reduction of learning rate - a condition where the learning rate is reduced to ensure better training when for a specified number of epochs a certain metric has stopped improving. The metric observed is validation accuracy to avoid the problem of overfitting. Along with this the training and validation accuracy and loss are saved to a csv file for future visualization. <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">EarlyStopping</span> <span class="n">Patience</span>      <span class="mi">3</span>
<span class="n">ReductionOfLR</span> <span class="n">Patience</span>      <span class="mi">2</span>
</code></pre></div> </div> </li> </ol> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/ZLfvswZ.png"/><br/> <i> Accuracy and Validation Accuracy compared for the various models </i> </p> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/tL7si9Q.png"/><br/> <i> Loss and Validation Loss compared for the various models </i> </p> <p>Observations from the graphs:</p> <ol> <li> <p>VGG16: The VGG16 model is unable to improve it’s validation accuracy for three epochs because it potentially fails to learn vitals features that help distinguish the COVID-19 positive and negative. As seen in the graph the training stops after three epochs and within those iterations it is evident that the model overfits on the training data as it is unable to perform adequately on the validation dataset.</p> </li> <li> <p>InceptionV3, Inception-ResNet, DenseNet121, and ResNet101: The InceptionV3 model has a large variance when validating on unseen data. Other models like Inception-ResNet, DenseNet121, and ResNet101 also have a large difference in the training and validation accuracies and losses.</p> </li> <li> <p>MobileNetV2: The MobileNetV2 model performs the best among the others. It has the highest training accuracy of about 95% and a validation accuracy of about 91%, while this still indicates overfitting, the extend of overfitting is least in this case. The same holds for losses, the difference in the training and validation loss is the least.</p> </li> </ol> <p>In summary, the MobileNet baseline model exhibited promising performance, with increasing accuracy and decreasing loss as training progressed. The validation accuracy and loss further demonstrated the model’s capability to generalize effectively. These results emphasize the suitability of the MobileNet architecture for the given task and dataset, and indicate the potential for further optimization or fine-tuning to enhance performance therefore now first bit plane slicing is performed and then thereafter train the MobileNetV2 model on the various extracted bit planes.</p> <h2 id="bit-plane-slicing">Bit Plane Slicing</h2> <p>Pixels are digital numbers that are comprised of bits. Instead of emphasizing the gray-level range, we choose to observe each bit’s contribution. It can be done using bit plane slicing. By isolating particular bits of the pixel values in an image, we can often highlight interesting aspects of that image. Higher-order bits usually contain most of the important visual information. Lower-order bits have subtle details.</p> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/f0zPK0K.png"/><br/> <i> Bit Plane Slicing for an 8 bit image </i> </p> <p>The image below shows the different planes obtained for a CXR, as we can observe the lower bit plane images are not visually informative, whereas the higher ones contain significant information. The lower bit planes have information that is not visually interpretable, but is identified by the DCNN as we will see in the sections ahead.</p> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/6Cr8c0V.png"/><br/> <i> Different Planes for a CXR </i> </p> <h2 id="results-and-discussions">Results and Discussions</h2> <p>Now that we’ve established the fact MobileNetV2 performs the best, we train the MobileNetV2 model on the bit plane sliced images as shown above.</p> <ul> <li>Bit Plane 0: We observe that the only keeping the least significant bit from each pixel, cause the model to overfit on the data as there isn’t enough visual information to learn from. The train accuracy is about 0.94 and the validation accuracy is appropriately 0.84, which validates the hypothesis of that the model overfits the data. Even when considering the loss the validation loss is almost twice the training loss.</li> </ul> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/vQghlgk.png"/><br/> <i> Bit Plane 0 (Training and Validation Accuracy and Loss) </i> </p> <ul> <li>Bit Plane 1: An interesting observation regarding this model is that is runs only for 3 epochs, we again implies that the lack of information from this bit plane affects the training of the model significantly as compared to the bit plane 0. The validation accuracy, decreases, and is lesser as compared to the training accuracy. The validation loss is greater than the training loss. Hence, here as well the model overfits on the data due to lack of visual information.</li> </ul> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/bMHei5f.png"/><br/> <i> Bit Plane 1 (Training and Validation Accuracy and Loss) </i> </p> <ul> <li>Bit Plane 2: The training done for bit plane 2 is quite peculiar and odd as it reaches an accuracy of 100% however we know it is not possible to attain such great accuracy in three epochs and with less data. I have not been able to come up with an explanation as to why this problem occurs, even after trying to re-train the data after extracting the data one more time, the error repeats itself.</li> </ul> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/qjmI568.png"/><br/> <i> Bit Plane 2 (Training and Validation Accuracy and Loss) </i> </p> <ul> <li>Bit Plane 3: Bit Plane 3 has results similar to Bit Plane 0.</li> </ul> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/iG3r4At.png"/><br/> <i> Bit Plane 3 (Training and Validation Accuracy and Loss) </i> </p> <ul> <li>Bit Plane 4: As seen by the graph the gap between the validation accuracy and training accuracy decreases, implying that as bit planes increase the level of overfitting decreases. The training and validation loss also follow this trend.</li> </ul> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/oCsJHTM.png"/><br/> <i> Bit Plane 4 (Training and Validation Accuracy and Loss) </i> </p> <ul> <li>Bit Plane 5: Even though the bit plane 5 model trains for three epochs and overfits on the data, it performs well.</li> </ul> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/mrzMNCK.png"/><br/> <i> Bit Plane 5 (Training and Validation Accuracy and Loss) </i> </p> <ul> <li>Bit Plane 6 &amp; 7: Both these bit planes overfit on the data, they show patterns similar to the the first two bit planes, however the validation and training losses are quite high in these models. This potentially be because of lack of information.</li> </ul> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/Yddw4me.png"/><br/> <i> Bit Plane 6 (Training and Validation Accuracy and Loss) </i> </p> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/vjJQNhK.png"/><br/> <i> Bit Plane 7 (Training and Validation Accuracy and Loss) </i> </p> <p>It would be fair to conclude that individual planes when provided as input to the DCNN overfits on the data. Hence training the entire image without any extraction would be a useful tool, fortunately a lot of work has been many works on the same, listed below in the references section.</p> <h2 id="code">Code</h2> <p>The code is written in Python 3, and run on a jupyter notebook, which can be accessed here - <a href="https://github.com/AmiteshBadkul/cxr-bit-plane/tree/master/code">CXR COVID</a>.</p> <h2 id="references">References</h2> <ol> <li><a href="https://link.springer.com/article/10.1007/s10489-020-01829-7">Classification of COVID-19 in chest X-ray images using DeTraC deep convolutional neural network</a></li> <li><a href="https://www.sciencedirect.com/science/article/pii/S0169260720309664">COVID-19 identification in chest X-ray images on flat and hierarchical classification scenarios</a></li> <li><a href="https://www.sciencedirect.com/science/article/pii/S0010482520301736">COVID-19 detection using deep learning models to exploit Social Mimic Optimization and structured chest X-ray images using fuzzy color and stacking approaches</a></li> <li><a href="https://www.sciencedirect.com/science/article/pii/S0010482520301621">Automated detection of COVID-19 cases using deep neural networks with X-ray images</a></li> <li><a href="COVID-Net: a tailored deep convolutional neural network design for detection of COVID-19 cases from chest X-ray images">COVID-Net: a tailored deep convolutional neural network design for detection of COVID-19 cases from chest X-ray images</a></li> </ol>]]></content><author><name>Amitesh Badkul</name></author><category term="computer-vision"/><category term="cv"/><category term="ml"/><category term="dl"/><summary type="html"><![CDATA[AI-based Tool]]></summary></entry><entry><title type="html">Computation of Mean Square Displacement and Diffusion Coefficient</title><link href="https://amiteshbadkul.github.io/blog/2022/msd/" rel="alternate" type="text/html" title="Computation of Mean Square Displacement and Diffusion Coefficient"/><published>2022-04-03T11:03:16+00:00</published><updated>2022-04-03T11:03:16+00:00</updated><id>https://amiteshbadkul.github.io/blog/2022/msd</id><content type="html" xml:base="https://amiteshbadkul.github.io/blog/2022/msd/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>After running an MD (Molecular Dynamic) Simulation, we obtain a large amount of data about the simulated system. This data can be used to gain more knowledge about the system. One of the primary advantages of MD simulations is that the extensive data obtained can predict various transport phenomena like viscosity, conductivity, and diffusivity. This work represents my attempt at analyzing the DCD files obtained after simulating a water box for 12 nanoseconds at 310 K. We utilized VMD to create the PSF files and NAMD to run the MD simulation. More details about the configuration file are at <a href="https://github.com/AmiteshBadkul/water-box">Water Box System</a>. We attempt to obtain the MSD (Mean Square Displacement) and subsequently the diffusion coefficient of the ensemble of molecules of the system. To learn how to run a simple simulation using a remote high performance computing cluster read <a href="https://amiteshbadkul.github.io/posts/2022/03/water-box-namd-vmd/">this</a>.</p> <h2 id="why-msd">Why MSD?</h2> <p>Mean Square Displacement for an MD simulation is defined as the measure of the change in a molecule’s position over time with respect to a reference point/position. One of the major reasons to calculate MSD is the calculation of the diffusivity (or the diffusion coefficient) of a molecule in a system. Knowing the diffusivity gives us knowledge about how the mobility of that particular molecule in the system. Diffusion is the process by which material is transported by the random thermal motion of the molecules within the fluid, even in the absence of any mean flow. For example, if a molecule has low diffusivity, then one can infer that the system could be highly viscous for that particular molecule.</p> <p>Equation for calculation of diffusion coefficient : <img src="https://latex.codecogs.com/svg.image?D&space;=&space;\frac{&lt;R^2&gt;}{6t}" alt="equation"/></p> <h2 id="preprocessing-data">Preprocessing Data</h2> <p>Once the MD simulation is completed, VMD provides an option (‘save coordinates’) to export the values of the coordinates of all the molecules or specific molecules. The trajectory (DCD) files and the structure (PSF) files should be loaded. Then we use this option to get the oxygen coordinate data for MSD analysis. We export the coordinate files in ‘xyz’ format. The coordinates obtained are in angstrom. Now Python is used to read the coordinate files for further analysis.</p> <p>The figure below shows how the exported coordinate file looks.</p> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/zfLdPVm.png"/><br/> <i> coordinate file </i> </p> <p>The start of the file shows the number of molecules in the system and how the file has been generated. In our case, we have 1981 molecules, and VMD has generated the coordinate files. This information is displayed in the file after every frame. Since this information is redundant for our analysis, it is removed from the file, and a new file is created by running the following code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Read in the file
</span><span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">'</span><span class="s">oxygen.xyz</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="nb">file</span> <span class="p">:</span>
  <span class="n">filedata</span> <span class="o">=</span> <span class="nb">file</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>


<span class="c1"># reading the first two lines to get knowledege about the atoms present
</span><span class="n">lines</span> <span class="o">=</span> <span class="n">filedata</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>
<span class="n">num_mol</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">lines</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">str_to_rep</span> <span class="o">=</span> <span class="n">lines</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="sh">'</span><span class="se">\n</span><span class="sh">'</span> <span class="o">+</span> <span class="n">lines</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="sh">'</span><span class="se">\n</span><span class="sh">'</span>

<span class="c1"># Replace the target string
</span><span class="n">filedata</span> <span class="o">=</span> <span class="n">filedata</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="n">str_to_rep</span><span class="p">,</span> <span class="sh">''</span><span class="p">)</span>

<span class="c1"># Write the file out again
</span><span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">'</span><span class="s">oxygen_edited.xyz</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">w</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="nb">file</span><span class="p">:</span>
  <span class="nb">file</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">filedata</span><span class="p">)</span>
</code></pre></div></div> <p>If we were only to observe the coordinates of the first molecule, we would see that after every 1981 row, the coordinate of the next time step is given. This is validated by the ‘num_mol’ variable, which contains the 1981 value. The ‘oxygen.xyz’ is the original file, and after removal of the undesired information, a new file, ‘oxygen_edited.xyz’, is written and contains only the coordinates and the molecule name.</p> <h2 id="analysis">Analysis</h2> <p>To read and perform a preliminary analysis of the preprocessed coordinate files, we utilize the Pandas, Matplotlib, and Seaborn libraries. First, we obtain information on the coordinates of the first molecule. Since we know the total number of rows and the total number of molecules, we can acquire the total number of frames by dividing the number of rows by the total number of molecules. So now we have the total number of frames, we can validate the number of coordinates obtained for the first molecule. To get only the data for the first molecule, we store the value and skip 1981 rows as we iterate through the whole dataset.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">getFirstMoleculeData</span><span class="p">(</span><span class="n">db</span><span class="p">,</span> <span class="n">num_mol</span><span class="p">):</span>
  <span class="sh">'''</span><span class="s">
  input
  db --&gt; the original dataset with undesired information removed
  num_mol --&gt; total number of molecules

  output
  Returns the coordinate data of the first molecule as pandas dataframe
  </span><span class="sh">'''</span>
  <span class="n">totalRows</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">db</span><span class="p">)</span>

  <span class="n">coordX</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">coordY</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">coordZ</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">totalRows</span><span class="p">,</span> <span class="n">num_mol</span><span class="p">):</span>

    <span class="n">coordX</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">db</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">coordY</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">db</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">coordZ</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">db</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">3</span><span class="p">])</span>


  <span class="n">df_final</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">X</span><span class="sh">'</span><span class="p">:</span> <span class="n">coordX</span><span class="p">,</span> <span class="sh">'</span><span class="s">Y</span><span class="sh">'</span><span class="p">:</span> <span class="n">coordY</span><span class="p">,</span> <span class="sh">'</span><span class="s">Z</span><span class="sh">'</span><span class="p">:</span> <span class="n">coordZ</span><span class="p">})</span>

  <span class="k">return</span> <span class="n">df_final</span>
</code></pre></div></div> <p>Now that we have successfully read and stored the data, we explore the trajectory of the oxygen atom of the first water molecule in the X, Y, and Z directions. Matplotlib and Seaborn libraries are utilized to plot the trajectory for the same.</p> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/O9VEUsN.png"/><br/> <i> X coordinates vs Time (in nanoseconds) </i> </p> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/XR5DmI2.png"/><br/> <i> Y coordinates vs Time (in nanoseconds) </i> </p> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/36qIFws.png"/><br/> <i> Z coordinates vs Time (in nanoseconds) </i> </p> <p>We extend the same function for one molecule to get data for all the molecules. We implement the following to obtain the data for all the molecules of the system.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">getAMoleculeData</span><span class="p">(</span><span class="n">db</span><span class="p">,</span> <span class="n">num_mol</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
  <span class="sh">'''</span><span class="s">
  input
  db --&gt; the dataset
  num_mol --&gt; total number of molecules
  idx --&gt; the molecule who</span><span class="sh">'</span><span class="s">s data is to be extracted

  output
  Returns the coordinate data of a particular molecule as a pandas dataframe
  </span><span class="sh">'''</span>
  <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Processing the data of molecule: </span><span class="sh">'</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
  <span class="n">totalRows</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">db</span><span class="p">)</span>

  <span class="n">coordX</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">coordY</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">coordZ</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">totalRows</span><span class="p">,</span> <span class="n">num_mol</span><span class="p">):</span>
    <span class="c1"># print(i)
</span>    <span class="n">coordX</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">db</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">coordY</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">db</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">coordZ</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">db</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">3</span><span class="p">])</span>

  <span class="n">diff</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">diff</span><span class="p">(</span><span class="n">disp_R</span><span class="p">,</span> <span class="n">append</span> <span class="o">=</span> <span class="n">disp_R</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="c1">#this calculates r(t + dt) - r(t)
</span>  <span class="n">diff_sq</span> <span class="o">=</span> <span class="n">diff</span><span class="o">**</span><span class="mi">2</span>

  <span class="n">df_final</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">X</span><span class="sh">'</span><span class="p">:</span> <span class="n">coordX</span><span class="p">,</span> <span class="sh">'</span><span class="s">Y</span><span class="sh">'</span><span class="p">:</span> <span class="n">coordY</span><span class="p">,</span> <span class="sh">'</span><span class="s">Z</span><span class="sh">'</span><span class="p">:</span> <span class="n">coordZ</span><span class="p">})</span>

  <span class="k">return</span> <span class="n">df_final</span>

<span class="k">def</span> <span class="nf">getAllMolData</span><span class="p">(</span><span class="n">db</span><span class="p">,</span> <span class="n">num_mol</span><span class="p">):</span>
  <span class="n">mols_data</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_mol</span><span class="p">):</span>
    <span class="n">mols_data</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">getAMoleculeData</span><span class="p">(</span><span class="n">db</span><span class="p">,</span> <span class="n">num_mol</span><span class="p">,</span> <span class="n">i</span><span class="p">))</span>

  <span class="k">return</span> <span class="n">mols_data</span>
</code></pre></div></div> <h2 id="msd-calculation">MSD Calculation</h2> <p>Since we have the data for all the molecules over the whole simulation duration, we now compute the MSD. The Algorithm implemented for MSD is shown in the figure below.</p> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/J7CB1j4.png"/><br/> <i> MSD Calculation </i> </p> <p>Code for the same is shown below.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">getMSD</span><span class="p">(</span><span class="n">db</span><span class="p">):</span>
  <span class="n">msd_data</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="k">for</span> <span class="n">iterone</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">db</span><span class="p">)):</span>
    <span class="nb">sum</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">itertwo</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">iterone</span><span class="p">):</span>
      <span class="nb">sum</span> <span class="o">=</span> <span class="nb">sum</span> <span class="o">+</span> <span class="n">db</span><span class="p">[</span><span class="sh">'</span><span class="s">SquareDiff</span><span class="sh">'</span><span class="p">][</span><span class="n">itertwo</span><span class="p">]</span>

    <span class="n">msd_data</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nb">sum</span><span class="p">)</span>

  <span class="n">db_final</span> <span class="o">=</span> <span class="n">db</span>
  <span class="n">db_final</span><span class="p">[</span><span class="sh">'</span><span class="s">MSD</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">msd_data</span>

  <span class="k">return</span> <span class="n">db_final</span>
</code></pre></div></div> <p>We extend this algorithm to get the MSD for all molecules and finally calculate the overall MSD.</p> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/J7CB1j4.png"/><br/> <i> MSD (for one molecule) (in Angstom) vs Time (in nanoseconds) </i> </p> <p>Equation for calculation of diffusion coefficient : <img src="https://latex.codecogs.com/svg.image?D&space;=&space;\frac{&lt;R^2&gt;}{6t}" alt="equation"/></p> <p>To calculate the diffusion coefficient, we use the formula mentioned above. We know that the diffusion coefficient is the slope of the equation relating MSD to time. Since we already have the data for MSD and time, we try to obtain the line of the best fit line without an intercept that passes through the origin (0, 0) and get the value of the slope.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span> <span class="o">=</span> <span class="n">df_msd</span><span class="p">[</span><span class="sh">'</span><span class="s">GlobalMSD</span><span class="sh">'</span><span class="p">]</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">df_msd</span><span class="p">[</span><span class="sh">'</span><span class="s">time</span><span class="sh">'</span><span class="p">]</span>
<span class="c1"># We only need a*x. so to figure out a we use lstsq from numpy
# Our x matrix is one dimensional, it needs to be two dimensional to use lstsq so:
</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span><span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">]</span>
<span class="n">a</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">lstsq</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="sh">'</span><span class="s">b-</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="sh">'</span><span class="s">r-</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">y = </span><span class="si">{</span><span class="n">a</span><span class="si">}</span><span class="s"> x + 0</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <table> <thead> <tr> <th style="text-align: center"><img src="https://i.imgur.com/lBw0rXB.png" alt="Overall MSD (in Angstom) vs Time (in nanoseconds) (Line of best fit)"/></th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><em>Overall MSD (in Angstom) vs Time (in nanoseconds) (Line of best fit)</em></td> </tr> </tbody> </table> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># the slope value we obtained after obtaining the line of best fit
</span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1078.36350041</span><span class="p">]</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">0</span>
</code></pre></div></div> <p>Now, this slope value is in the unit Angstrom square per nanoseconds. We convert this into centimeter square per second as the diffusion coefficient is generally represented using centimeter square per second. After conversion and division by 6 (as given in the equation), we obtain a value of 1.793-centimeter square per second, which is close to the value obtained by Jorgensen et al. for the TIPS2 model of water at 293 K, that is 3.2-centimeter square per seconds. The possible reason for this difference in value could be running the system for very little time (around eight nanoseconds) and the difference in temperature since we ran our system at 310 K instead of 293 K.</p> <h2 id="code">Code</h2> <p>The code is written in Python 3, and run on a jupyter notebook, which can be accessed here - <a href="https://github.com/AmiteshBadkul/water-box/blob/master/analysis/MSD_Calculation.ipynb">Water Box System</a>.</p> <h2 id="references">References</h2> <ol> <li><a href="http://web.mit.edu/savin/Public/.Tutorial_v1.2/Concepts.html">Calculating Mean Square Displacement</a></li> <li><a href="https://amberhub.chpc.utah.edu/diffusion/">Diffusion</a></li> <li><a href="http://isaacs.sourceforge.net/phys/msd.html">Mean Square Displacement of atoms - M.S.D.</a></li> <li><a href="https://doi.org/10.1063/1.445869">Jorgensen, William L., et al. “Comparison of simple potential functions for simulating liquid water.” The Journal of chemical physics 79.2 (1983): 926-935</a></li> <li><a href="https://doi.org/10.33011/livecoms.1.1.6324">Maginn, Edward J., et al. “Best practices for computing transport properties 1. Self-diffusivity and viscosity from equilibrium molecular dynamics Living Journal of Computational Molecular Science 1.1 (2019): 6324-6324</a></li> </ol>]]></content><author><name>Amitesh Badkul</name></author><category term="molecular-dynamics"/><category term="md"/><category term="vmd"/><category term="namd"/><summary type="html"><![CDATA[MSD and DC analysis]]></summary></entry><entry><title type="html">Fundamentals of CYP3A4 Binding</title><link href="https://amiteshbadkul.github.io/blog/2022/cheminformatics/" rel="alternate" type="text/html" title="Fundamentals of CYP3A4 Binding"/><published>2022-03-25T09:23:16+00:00</published><updated>2022-03-25T09:23:16+00:00</updated><id>https://amiteshbadkul.github.io/blog/2022/cheminformatics</id><content type="html" xml:base="https://amiteshbadkul.github.io/blog/2022/cheminformatics/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>To understand the general flow of QSAR predictive modeling, I selected the problem of binary classification of the cytochrome P450 3A4 inhibition. In principle, binary classification is more straightforward to model and understand. Moreover, it is easier to identify the fallacies involved in creating the models and curating the datasets involved.</p> <p>CYP3A4 is an important enzyme in the body, mainly found in the liver and in the intestine. It oxidizes small foreign organic molecules, such as toxins or drugs, so that they can be removed from the body.</p> <p>The modeling done can be replicated for the other CYP450 enzymes as well.</p> <h2 id="dataset">Dataset</h2> <p>The dataset used for binary classification was created by Veith et al. to better predict the interaction of a large variety of drugs with the CYP450 enzymes.</p> <p>The dataset contains drugs from:</p> <ol> <li><a href="https://pubchem.ncbi.nlm.nih.gov/source/MLSMR">MLSMR</a> - contains chemically diverse small compounds based on Lipinski’s rule of five, synthetic tractability, and availability.</li> <li>6,144 compounds from a set of bio focused libraries, which included 1,114 FDA-approved drugs</li> <li>980 compounds from combinatorial libraries containing privileged structures targeted at GPCRs and kinases and libraries of purified natural products or related structure</li> </ol> <p>Understanding the selection criterion -</p> <ol> <li><strong>Lipinski’s rule of five</strong>: Lipinski’s rule of 5 helps distinguish between drug-like and non-drug-like molecules. Suppose the compound in question complies with at least two of the following rules. In that case, it is associated with a high probability of drug-likeness - Molecular mass less than 500 Dalton, high lipophilicity (expressed as LogP less than 5), less than five hydrogen bond donors, less than ten hydrogen bond acceptors, and molar refractivity should be between 40-130. If a drug satisfies all the constraints mentioned, it is highly likely to have good permeation and absorption. A drug’s permeability across biological membranes is a critical factor influencing absorption and distribution. If a drug wants to reach systemic circulation, it needs to cross several semipermeable cell membranes first. Hence, Lipinski’s rule of five is an important criterion.</li> <li><strong>Synthetic tractability</strong>: Medicinal chemists evaluate compounds according to their synthesis feasibility and other parameters such as up-scaling or cost of goods.</li> <li><strong>Focused Libraries</strong>: A target-focused library is a collection of compounds that have been either designed or assembled with a protein target or protein family in mind.</li> <li><strong>Combinatorial libraries</strong>: These are collections of chemical compounds, small molecules, or macromolecules such as proteins, synthesized by combinatorial chemistry, in which multiple different combinations of related chemical species are reacted together in similar chemical reactions.</li> </ol> <h3 id="dataset-curation">Dataset Curation</h3> <p>The dataset is downloaded from <a href="https://tdcommons.ai/single_pred_tasks/adme/#cyp-p450-3a4-inhibition-veith-et-al">TDCommons</a>. The TDCommons python library has the option of performing various types of splits including random, scaffold, cold-start, and combination split. We utilize the random and scaffold splits.</p> <h2 id="representation-of-the-compounds">Representation of the Compounds</h2> <p>Representation of molecules in the form of vectors is an active research field. A fingerprint is a type of molecular representation that is useful. It is essential to have a method that appropriately represents the molecule so that the machine learning models can accurately perform. Fingerprints exist in various shapes and sizes, and they’ve been used to solve a wide range of problems during the previous few decades. ECFPs (or “Circular Fingerprints”) provides various advantages over other methods.</p> <p>The advantages of using ECFP include:</p> <ol> <li>The ECFP method comprises fewer units, making it simple to implement.</li> <li>Multiple variations are possible on the base algorithm.</li> <li>The ECFP algorithm is computationally quick.</li> <li>Topological structural information is adequately represented.</li> </ol> <p>The Extended-connectivity fingerprint (ECFP) of bond diameter 4 for the drug compounds was obtained and converted into 1024 bit vector.</p> <p>To understand how exactly the algorithm for the ECFP fingerprint works consider reading the following:</p> <ol> <li><a href="https://chemicbook.com/2021/03/25/a-beginners-guide-for-understanding-extended-connectivity-fingerprints.html">ChemicBook Blog’s Simple Explanation</a></li> <li><a href="https://pubs.acs.org/doi/10.1021/ci100050t">Rogers and Hahn</a></li> </ol> <h2 id="chemical-space-exploration-and-analysis">Chemical Space Exploration and Analysis</h2> <p>The dataset’s chemical space was explored to understand better the various molecules present in the dataset as similar molecules are clustered together. In contrast, dissimilar ones are far from each other. We can also understand the similarity between the training, testing, and validation datasets. Principal component analysis or PCA is a commonly employed method to understand the chemical space. PCA is a dimensionality reduction technique that produces new variables using a linear combination of old variables that are better able to describe the dataset. We use the explained variance ratio metric to verify if PCA adequately retained the original variables’ information. The higher the ratio, the more variance from the data is retained. To visualize the chemical space, we reduce the variables to two using PCA.</p> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/3OHCq27.png"/><br/> <i> PC1 and PC2 are the linear combinations of the previous variables </i> </p> <p>PCA is a linear dimension reduction technique that cannot adequately explain the clustering of drugs. In contrast, T-distributed Stochastic Neighbor Embedding (t-SNE) is a non-linear dimensionality reduction technique that can explain the same. The math behind t-SNE is quite complex, but the idea is simple. It embeds the points from a higher dimension to a lower dimension trying to preserve the neighborhood of that point. t-SNE is better able to capture the extent of distribution of the compounds of the dataset.</p> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/keANY5J.png"/><br/> <i> The t-SNE plot is generated such that 95% of the variance is retained </i> </p> <p>To better understand PCA and t-SNE consider reading the following:</p> <ol> <li><a href="https://medium.com/swlh/t-sne-explained-math-and-intuition-94599ab164cf">Achinoam Soroker’s Simple Explanation</a></li> <li><a href="https://towardsdatascience.com/an-introduction-to-t-sne-with-python-example-5a3a293108d1">Andre Violante’s Explanation with python example</a></li> <li><a href="https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf">Maaten and Hinton</a></li> </ol> <h2 id="cleaning-the-dataset">Cleaning the Dataset</h2> <p>Removal of drug compounds from the dataset, as these drug compounds lead to model instability, with:</p> <ol> <li>Invalid SMILES string.</li> <li>Outlier removal using molecular weight as a descriptor - if the molecule has a weight three times the standard deviations from the mean, it was flagged as an outlier.</li> </ol> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/4kTGoKo.png"/><br/> <i> Histogram distribution after outlier removal </i> </p> <h2 id="classification-models">Classification Models</h2> <p>The following classification algorithms were deployed:</p> <ol> <li>Logistic Regression</li> <li>Random Forests Classifier</li> <li>XGBoost Classifier</li> </ol> <h3 id="evaluation-metrics">Evaluation Metrics</h3> <ol> <li>AUC - ROC (Area Under the Receiver Operating Characteristics) - It helps in quality comparison of various models, the higher the values, the better the model is with respect to distinguishing properly between the two classes.</li> <li>Matthews correlation coefficient (MCC) - is a balanced metric, which produces a high score only if the prediction obtained good results in all of the four confusion matrix categories (true positives, false negatives, true negatives, and false positives), proportionally both to the size of positive elements and the size of negative elements in the dataset. It is very suitable for binary classification. MCC provides a measure of the correlation between predictions and observations. A value of 1 represents a perfect agreement, whereas values of 0 and –1 correspond to random and perfect disagreement, respectively.</li> </ol> <h3 id="logistic-regression">Logistic Regression</h3> <p>For classification tasks, the logistic regression method is employed. It’s a probabilistic prediction analysis algorithm. The logistic regression classifier is intended to provide us with a set of outputs or classifications depending on likelihood. We run the data through a prediction algorithm and get a likelihood score between 0 and 1.</p> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/N7Z0HVF.png"/><br/> <i> AUC - ROC of Logistic Regression of training, testing and validation </i> </p> <p>Logistic Regression is a linear classifier, the decision boundary it generates is linear. Since the dependence between molecular descriptors and end points in non-linear, logistic regression overfits on the training data and is unable to predict with similar accuracy on the test and validation set.</p> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/0yav04k.png"/><br/> <i> MCC of the random and scaffold split for training, testing and validation </i> </p> <p>In case of both the random and scaffold split similar results are obtained - possibly due to the random and scaffold split having similar test and validation drug compounds.</p> <h3 id="random-forest-classifier">Random Forest Classifier</h3> <p>Tin Kam Ho introduced the general concept of Random Forests in the year 1995. It is an ensemble classification technique that has high accuracy. The error for classification decreases as the number of trees in the forest increases. Leo Breiman further established random forests more concretely in 2001, he defined random forests as a classifier that consists of a set of tree-based classifiers where each classifier has independent weights and independently casts votes for the most popular class of the data provided.</p> <p>Advantages of Random Forests include:</p> <ol> <li>Reduces overfitting to a large extent.</li> <li>Works on both classification as well as regression problems.</li> <li>Normalization of data is unessential because of the rule-based approach.</li> <li>Has better accuracy as compared to decision trees</li> </ol> <p>Disadvantages of Random Forests include:</p> <ol> <li>The process of training is computational expensive due to the large number of decision trees present.</li> <li>Time required for training is more when compared to decision trees.</li> </ol> <p align="center" width="100%"> <img width="75%" src="https://www.freecodecamp.org/news/content/images/2020/08/how-random-forest-classifier-work.PNG"/><br/> <i> Random Forest Classifier </i> </p> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/p5OrTMH.png"/><br/> <i> ROC of Random Forest Classifier of training, testing and validation </i> </p> <p>To overcome the issue of linear boundary generation for non-linear data, we use the Random Forest Classifier, a non-linear classifier. In this case, we also see that the model overfits the data, possibly due to producing exact non-linear boundaries for classification. Tuning of Hyperparameters in the RF classifier affects all the decision trees, so one needs to find the tradeoff between the value of the ROC AUC metric obtained and the changes in the hyperparameters.</p> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/ZcRsmT0.png"/><br/> <i> MCC of the random and scaffold split for training, testing and validation </i> </p> <p>The scaffold split train dataset has a higher MCC due to overfitting, whereas the random split model generalizes better. The scaffold split doesn’t generalize well on unseen data as the test and the validation set contains drugs with scaffolds different from the training set.</p> <h3 id="xgboost-classifier">XGBoost Classifier</h3> <p>Chen and Guestrin from the University of Washington introduced XGBoost in the year 2016. XGBoost is an ensemble learning technique based on decision trees. While there do exist gradient boosting algorithms, none of them are as widely used as XGBoost due to it’s scalability, and it’s execution time, it is computationally very quick. XGBoost is known to achieve high accuracy on tabular data. It has the same advantages as random forest with the added advantage that it is quicker. XGBoost optimizes the algorithm to produce such low computational speed and high accuracy.</p> <p>Some features of XGBoost include:</p> <ol> <li>Gradient-based Tree Boosting with L1/L2 Regularization.</li> <li>Construction of parallization on CPU cores.</li> <li>Distributed training on a cluster of machines for a large dataset.</li> <li>Out-of-Core computation for large datasets that cause storage problems.</li> </ol> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/yraKhUa.png"/><br/> <i> AUC - ROC of XGBoost Classifier of training, testing and validation </i> </p> <p>The XGBoost model can overcome the problem of overfitting to a certain extent, as evident by the ROC AUC values of the training, testing, and validation datasets. Even the MCC metric values indicate that there is not much difference between the training, testing, and validation results obtained.</p> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/k90hUCS.png"/><br/> <i> MCC of the random and scaffold split for training, testing and validation </i> </p> <h2 id="discussion">Discussion</h2> <p>Some potential reasons for values of these metrics being moderate and not higher include -</p> <ol> <li>1024 number of bits for the representation of drug molecules don’t fully capture the structure of molecules.</li> <li>The ECFP radii as 2 are not able to properly represent the molecule.</li> <li>Maybe the ECFP doesn’t adequately represent the molecules and a different molecular descriptor like Avalon Fingerprints, 2D Pharmacophore Fingerprints, etc.</li> </ol> <h2 id="future-work">Future Work</h2> <p>Future work includes:</p> <ol> <li>Further Evaluation of the results utilizing chemical space analysis on the predicted values and comparing the same to actual results to understand which particular molecule affects the model.</li> <li>Implementation of Deep Learning-based techniques such as Graph Convolutional Neural Networks and Multilayer Perceptron (MLP).</li> <li>Implementation of different descriptors for the drugs.</li> </ol> <h2 id="code">Code</h2> <p>The code for the project is available <a href="https://github.com/AmiteshBadkul/CYP3A4">here</a>.</p> <h2 id="references">References</h2> <ol> <li><a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/minf.201000061">Tropsha, Alexander. “Best practices for QSAR model development, validation, and exploitation.” Molecular informatics 29.6‐7 (2010): 476-488</a></li> <li><a href="https://www.mdpi.com/1420-3049/26/15/4678">Holmer, Malte, et al. “CYPstrate: A Set of Machine Learning Models for the Accurate Classification of Cytochrome P450 Enzyme Substrates and Non-Substrates.” Molecules 26.15 (2021): 4678</a></li> <li><a href="https://www.sciencedirect.com/science/article/pii/S1359644620302609">Göller, Andreas H., et al. “Bayer’s in silico ADMET platform: A journey of machine learning over the past two decades.” Drug Discovery Today 25.9 (2020): 1702-1709</a></li> <li><a href="https://tdcommons.ai/single_pred_tasks/adme/#cyp-p450-3a4-inhibition-veith-et-al">Dataset</a></li> <li><a href="https://ieeexplore.ieee.org/document/598994">Ho, Tin Kam. “Random decision forests.” Proceedings of 3rd international conference on document analysis and recognition. Vol. 1. IEEE, 1995</a></li> <li><a href="https://dl.acm.org/doi/abs/10.1145/2939672.2939785">Chen, Tianqi, and Carlos Guestrin. “Xgboost: A scalable tree boosting system.” Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining. 2016</a></li> </ol>]]></content><author><name></name></author><category term="cheminformatics"/><category term="cheminformatics"/><category term="ml"/><category term="dl"/><summary type="html"><![CDATA[CYP3A4 inhibition prediction]]></summary></entry><entry><title type="html">Learnings from Molecular Dynamic Simulation</title><link href="https://amiteshbadkul.github.io/blog/2022/md-simulation/" rel="alternate" type="text/html" title="Learnings from Molecular Dynamic Simulation"/><published>2022-03-17T16:40:16+00:00</published><updated>2022-03-17T16:40:16+00:00</updated><id>https://amiteshbadkul.github.io/blog/2022/md-simulation</id><content type="html" xml:base="https://amiteshbadkul.github.io/blog/2022/md-simulation/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>To understand the process of creating and running a molecular dynamic simulation using <a href="https://www.ks.uiuc.edu/Research/namd/">NAMD</a> and <a href="https://www.ks.uiuc.edu/Research/vmd/">VMD</a>, I ran a simple simulation of a water box. The first half of the post discusses MD Simulations and their sophistication and intricacies. The second half discusses creating and simulating a water box using a high-performance computing cluster.</p> <h2 id="computer-simulations">Computer Simulations</h2> <p>Computer simulations help us better analyze the properties of molecular assemblies in terms of microscopic interactions and structure of the same. This works as a supplement to traditional experiments, empowering us to learn something new that we couldn’t learn any other way. Molecular dynamics (MD) and Monte Carlo are the most common simulation techniques (MC). Computer simulations help link time scales and the microscopic length with the macroscopic world in the laboratory. It provides a view of the interactions among molecules and obtains accurate predictions of the bulk properties. Molecular dynamics (MD) is a procedure where the motion of the particles of a system over a specific period is recorded by solving Newton’s equations of motion. This is done for each particle in the system over the simulation duration. However, the MD simulation method was introduced in the 1950s by Marshall Rosenbluth and Nicholas Metropolis in the form of the Metropolis-Hastings algorithm.</p> <table> <thead> <tr> <th style="text-align: center"><img src="https://upload.wikimedia.org/wikipedia/commons/7/7b/Molecular_dynamics_algorithm.png" width="50%" height="50%"/></th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><em>The general procedure for MD Simulation is illustrated</em></td> </tr> </tbody> </table> <h3 id="namd">NAMD</h3> <p>NAMD was first introduced by Nelson et al., who was part of the theoretical biophysics group, in the year 1995. It was implemented and created using C++. Currently, the theoretical biophysics group and the parallel programming laboratory at the University of Illinois develop, upgrade, and maintain NAMD. NAMD is used to perform dynamic molecular simulations. For any simulation, the input files required and their description are given below.</p> <ol> <li>PDB (Protein Data Bank) - Contains information on the coordinates of atoms of the Protein</li> <li>PSF (Protein Structure File) - Contains all the information necessary to convert a list of residual names into a complete PSF structure file. Aids with the automatic designation of hydrogen atoms to the protein</li> <li>Topology file - Contains all the molecular-specific information needed to apply a particular force field to the molecular system</li> <li>Parameter file - Contains all the numerical constants necessary to evaluate energies, forces, speed, and position of the atoms of the protein</li> </ol> <h3 id="vmd">VMD</h3> <p>The theoretical and computational biophysics group established VMD under the supervision of Klaus Schulten. VMD has Tcl (a high-level programming language) and Python interpreters. VMD is used along with NAMD to visualize the proteins and the steps involved in the simulation. The theoretical and computational biophysics group currently maintains it. VMD recognizes various files of molecular data for visualization purposes. The list below gives more information about the same.</p> <ol> <li>Protein Data Bank files</li> <li>X-PLOR and Chemistry at Harvard Macromolecular Mechanics (CHARMM) PSF files</li> <li>X-PLOR and Chemistry at Harvard Macromolecular Mechanics (CHARMM) trajectory DCD files</li> <li>X-PLOR and Chemistry at Harvard Macromolecular Mechanics (CHARMM) trajectory DCD files</li> <li>Assisted Model Building with Energy Refinement (AMBER) trajectory and structure files</li> <li>GROningen MAchine for Chemical Simulations (GROMACS) trajectory and structure files</li> </ol> <h2 id="molecular-dynamics-simulation">Molecular Dynamics Simulation</h2> <p>Computer simulations help us better analyze the properties of molecular assemblies in terms of microscopic interactions and structure of the same. This works as a supplement to traditional experiments, empowering us to learn something new that we couldn’t learn any other way. Molecular dynamics (MD) and Monte Carlo are the most common simulation techniques (MC). Computer simulations help link time scales and the microscopic length with the macroscopic world in the laboratory. It provides a view of the interactions among molecules and obtains accurate predictions of the bulk properties. Molecular dynamics (MD) is a procedure where the motion of the particles of a system over a specific period is recorded by solving Newton’s equations of motion. This is done for each particle in the system over the simulation duration.</p> <p>Characteristics of Molecular Dynamic Simulation include the following:</p> <ol> <li>It is a deterministic technique</li> <li>It only takes into consideration the the intermolecular forces and the starting position of each particle in the system</li> <li>It can be successfully be used as a statistical mechanics method</li> <li>The data recorded about the distances and the velocities can be averaged over time and over all the molecules of the system to obtain the thermodynamic quantities of the system</li> <li>It provides atomic and molecular level information on the system</li> </ol> <table> <thead> <tr> <th style="text-align: center"><img src="https://i.imgur.com/zXjH2ec.png" width="70%"/></th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><em>The Steps Involved in Molecular Dynamic Simulation</em></td> </tr> </tbody> </table> <p>The main disadvantage of Molecular Dynamic Simulation is that the result is often an approximate measure because solving the Schrodinger equation to obtain the wavefunction that accurately describes atom/particle is computationally expensive and time-consuming. However, the error obtained from simulation can be minimized by choosing the correct potential function. A wide range of potential functions makes the simulation results realistic and more precise.</p> <h3 id="the-algorithm">The Algorithm</h3> <p>Newton’s equation of motions are solved for all the system particles for all molecular dynamic simulations.</p> <p>As input we need only the following:</p> <ol> <li>The starting set of coordinates of the atoms of a system.</li> <li>The set of potential functions for obtaining the forces among various atoms/particles of the system.</li> </ol> <p>The five quintessential components for any Molecular Dynamic Simulation are initial condition, boundary condition, integrator/ensemble, force calculation, and property calculation. The distinct properties calculated include - dynamic structure factor and thermal conductivity, the viscosity of the system particles, internal energy, free energy, enthalpy, and many more. The various potentials include - the Lennard-Jones potential Coulomb–Buckingham potential.</p> <h4 id="lennard-jones-potential">Lennard-Jones Potential</h4> <p>It is an intermolecular pair potential. Among all the intermolecular pairs, researchers have extensively examined the Lennard-Jones potential.</p> <p align="center" width="100%"> <img width="50%" src="https://upload.wikimedia.org/wikipedia/commons/9/93/12-6-Lennard-Jones-Potential-equation.svg"/><br/> <i> Lennard-Jones Potential Expression </i> </p> <p align="center" width="100%"> <img width="50%" src="https://upload.wikimedia.org/wikipedia/en/e/e7/Graph_of_Lennard-Jones_potential.png"/><br/> <i> Lennard-Jones Potential Graph </i> </p> <h4 id="coulombbuckingham-potential">Coulomb–Buckingham Potential</h4> <p>It is a modification of the Buckingham Potential, which was put forth by Richard Buckingham. For the application of the Buckingham potential to ionic systems, the modification was made, and hence the new potential came to be known as Coulomb-Buckingham Potential.</p> <p align="center" width="100%"> <img width="50%" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/80e3fc27abeca3b8e89135ee679c5c10bdedc9d1"/><br/> <i> Coulomb–Buckingham Potential Expression </i> </p> <p align="center" width="100%"> <img width="50%" src="https://upload.wikimedia.org/wikipedia/commons/b/b1/Coulomb-Buckingham_Potential.png"/><br/> <i> Coulomb–Buckingham Potential Graph </i> </p> <h2 id="water-box-simulation">Water Box Simulation</h2> <p>The water box is modelled by utilizing the VMD software. We use the “Add Solvation Box” command to generate the water box. The command can be accessed as follows:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">VMD</span> <span class="o">&gt;</span> <span class="n">Extensions</span> <span class="o">&gt;</span> <span class="n">Modeling</span> <span class="o">&gt;</span> <span class="n">Add</span> <span class="n">Solvation</span> <span class="n">Box</span>
</code></pre></div></div> <p>The command generates the PDB (Protein Data Bank) and the PSF (Protein Structure File) for the water box after taking in the coordinates of the box, for our case we provide 40nm as the length of our water box, centered at (0, 0, 0). The diagram below depicts the PDB of water box generated as displayed by VMD.</p> <p align="center" width="100%"> <img width="50%" src="https://i.imgur.com/bObWAFi.png"/><br/> <i> Water Cube </i> </p> <p>The first few lines of the PDB file for the water box:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">CRYST1</span>   <span class="mf">40.000</span>   <span class="mf">40.000</span>   <span class="mf">40.000</span>  <span class="mf">90.00</span>  <span class="mf">90.00</span>  <span class="mf">90.00</span> <span class="n">P</span> <span class="mi">1</span>           <span class="mi">1</span>
<span class="n">ATOM</span>      <span class="mi">1</span>  <span class="n">OH2</span> <span class="n">TIP3W</span>   <span class="mi">5</span>     <span class="o">-</span><span class="mf">16.332</span>  <span class="o">-</span><span class="mf">9.918</span>  <span class="o">-</span><span class="mf">4.096</span>  <span class="mf">1.00</span>  <span class="mf">0.00</span>      <span class="n">WT1</span>  <span class="n">O</span>
<span class="n">ATOM</span>      <span class="mi">2</span>  <span class="n">H1</span>  <span class="n">TIP3W</span>   <span class="mi">5</span>     <span class="o">-</span><span class="mf">16.776</span>  <span class="o">-</span><span class="mf">9.549</span>  <span class="o">-</span><span class="mf">4.899</span>  <span class="mf">1.00</span>  <span class="mf">0.00</span>      <span class="n">WT1</span>  <span class="n">H</span>
<span class="n">ATOM</span>      <span class="mi">3</span>  <span class="n">H2</span>  <span class="n">TIP3W</span>   <span class="mi">5</span>     <span class="o">-</span><span class="mf">16.908</span>  <span class="o">-</span><span class="mf">9.621</span>  <span class="o">-</span><span class="mf">3.373</span>  <span class="mf">1.00</span>  <span class="mf">0.00</span>      <span class="n">WT1</span>  <span class="n">H</span>
<span class="n">ATOM</span>      <span class="mi">4</span>  <span class="n">OH2</span> <span class="n">TIP3W</span>   <span class="mi">7</span>     <span class="o">-</span><span class="mf">13.967</span> <span class="o">-</span><span class="mf">15.124</span>   <span class="mf">0.891</span>  <span class="mf">1.00</span>  <span class="mf">0.00</span>      <span class="n">WT1</span>  <span class="n">O</span>
<span class="n">ATOM</span>      <span class="mi">5</span>  <span class="n">H1</span>  <span class="n">TIP3W</span>   <span class="mi">7</span>     <span class="o">-</span><span class="mf">13.922</span> <span class="o">-</span><span class="mf">14.776</span>   <span class="mf">1.798</span>  <span class="mf">1.00</span>  <span class="mf">0.00</span>      <span class="n">WT1</span>  <span class="n">H</span>
<span class="n">ATOM</span>      <span class="mi">6</span>  <span class="n">H2</span>  <span class="n">TIP3W</span>   <span class="mi">7</span>     <span class="o">-</span><span class="mf">13.408</span> <span class="o">-</span><span class="mf">15.912</span>   <span class="mf">0.961</span>  <span class="mf">1.00</span>  <span class="mf">0.00</span>      <span class="n">WT1</span>  <span class="n">H</span>    
</code></pre></div></div> <p>Now that the PDB and the PSF files are ready, we need to create the configuration file that NAMD uses to set parameters for the MD simulation that needs to be run. The various parameters necessary for running a simulation are - structure file, coordinate file, temperature, restartfreq, dcdfreq, xstFreq, outputEnergies, force-field parameters, cutoff, stepspercycle, timestep, langevin, langevinDamping, langevinTemp, langevinHydrogen, periodic boundary conditions, particle mesh Ewald (PME), langevinPiston, langevinPistonTarget, langevinPistonPeriod, langevinPistonDecay, langevinPistonTemp, minimize, and run.</p> <p>This section explores each of these parameters in detail.</p> <ol> <li>Structure - This takes as input the relative location of the structure file, in our case the PSF file generated by VMD. <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">structure</span>       <span class="p">..</span><span class="o">/</span><span class="nb">input</span><span class="o">/</span><span class="n">solvate</span><span class="p">.</span><span class="n">psf</span>
</code></pre></div> </div> </li> <li>Coordinates - Just like the structure parameter, this takes the relative location of the PDB file, which the coordinates of the structure. <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">coordinate</span>      <span class="p">..</span><span class="o">/</span><span class="nb">input</span><span class="o">/</span><span class="n">solvate</span><span class="p">.</span><span class="n">pdb</span>
</code></pre></div> </div> </li> <li>Temperature - Sets the temperature for the MD simulation (in Kelvin). <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">temperature</span>     <span class="mi">300</span>
</code></pre></div> </div> </li> <li>. Restart Frequency, DCD Frequency, XST Frequency, Output Energies - The frequency with which the restart files, DCD (coordinates), and the XST (velocity) files are generated. Output Energies rewrites the output files every fixed number of steps. <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">restartfreq</span>     <span class="mi">1000</span> <span class="c1"># 1000 steps = 2ps
</span><span class="n">dcdfreq</span>         <span class="mi">1000</span>
<span class="n">xstFreq</span>         <span class="mi">1000</span>
<span class="n">outputEnergies</span>  <span class="mi">150</span>
</code></pre></div> </div> </li> <li>Force Field Parameters - paraTypeCharmm and parameters - The paraTypeCharmm parameter checks whether the parameters file is CHARMM type, and the parameter keyword takes the relative location of the force field parameter file. <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">paraTypeCharmm</span>  <span class="n">on</span>
<span class="n">parameters</span>      <span class="p">..</span><span class="o">/</span><span class="nb">input</span><span class="o">/</span><span class="n">par_all27_prot_lipid</span><span class="p">.</span><span class="n">inp</span>
</code></pre></div> </div> </li> <li>Cutoff - NAMD effectively removes the Van Der Waal forces of interaction by the specified cutoff value. The Fig. 2.2 illustrates the working of the Cutoff parameter. <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cutoff</span>          <span class="mi">10</span>
</code></pre></div> </div> </li> <li>Steps per cycle - The total number of steps taken for completion of one cycle is specified by the stepspercycle parameter. <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">stepspercycle</span>   <span class="mi">5</span>
</code></pre></div> </div> </li> </ol> <p align="center" width="100%"> <img width="50%" src="https://www.ks.uiuc.edu/Research/namd/2.10/ug/img74.png"/><br/> <i> Illustration of the working of the Cutoff parameter </i> </p> <ol> <li>Time Step - The time taken to complete one step, specified in femtoseconds. <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">timestep</span>        <span class="mf">2.0</span>
</code></pre></div> </div> </li> <li>Temperature Control Parameters - The ‘langevin’ parameter specifies whether the Langevin dynamics are to be utilized. ‘langevinDamping’ specifies the damping coefficient for the said dynamics. ‘langevinTemp’ states the temperature in Kelvin for langevin dynamics. ‘langevinHydrogen’ tells NAMD whether to apply Langevin dynamics to hydrogen atoms present. <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">langevin</span>         <span class="n">on</span>
<span class="n">langevinDamping</span>  <span class="mi">1</span>
<span class="n">langevinTemp</span>     <span class="mi">300</span>
<span class="n">langevinHydrogen</span> <span class="n">no</span>
</code></pre></div> </div> </li> <li>Periodic Boundary Conditions - The parameters cellBasisVector1, cellBasisVector2, and cellBasisVector3 state the length of the repeating units. This is implemented to avoid the finite size problem and to make the system an infinite one for smoother simulation. cellOrigin - takes as input the center/origin of the simulation. <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cellBasisVector1</span> <span class="mi">40</span>   <span class="mi">0</span>   <span class="mi">0</span>
<span class="n">cellBasisVector2</span> <span class="mi">0</span>    <span class="mi">40</span>  <span class="mi">0</span>
<span class="n">cellBasisVector3</span> <span class="mi">0</span>    <span class="mi">0</span>   <span class="mi">40</span>
<span class="n">cellOrigin</span>       <span class="mi">0</span>    <span class="mi">0</span>   <span class="mi">0</span>
</code></pre></div> </div> </li> <li>Pressure Control - The ‘langevinPiston’ specifies whether the Langevin piston is to be used or not. ‘langevinPistonTarget’ specifies the target pressure required for the calculations involved in piston method, in bar. ‘langevinPistonPeriod’ specifies the duration for the piston method, in femtoseconds. ‘langevinPistonDecay’ - specifies the damping time period for the method, in femtoseconds. ‘langevinPistonTemp’ specifies the piston temperature, in Kelvin.</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">useGroupPressure</span>       <span class="n">yes</span>
<span class="n">useFlexibleCell</span>        <span class="n">no</span>
<span class="n">useConstantArea</span>        <span class="n">no</span>
<span class="n">LangevinPiston</span>         <span class="n">on</span>
<span class="n">LangevinPistonTarget</span>   <span class="mf">1.01325</span>
<span class="n">LangevinPistonPeriod</span>   <span class="mi">100</span>
<span class="n">LangevinPistonDecay</span>    <span class="mi">50</span>
<span class="n">LangevinPistonTemp</span>     <span class="mf">300.0</span>
</code></pre></div></div> <ol> <li>PME or Particle Mesh Ewald is used along side with periodic boundary conditions. ‘PMEGridSpacing’ specifies the distance between the grids.</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">PME</span>             <span class="n">yes</span>
<span class="n">PMEGridSpacing</span>  <span class="mi">2</span>
</code></pre></div></div> <ol> <li>Minimize - Performs the process of minimization, which refers to the process of changing the spatial arrangement of the atoms to lower the energy and to prevent bad contacts from occurring as soon as the simulation starts, for specified number of steps. <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">minimize</span>       <span class="mi">5000</span>
</code></pre></div> </div> </li> <li>Run - The Run commands starts the simulation for a specified number of steps.</li> </ol> <p>In our case since we have to submit our job to a cluster we define multiple configuration files, this is done to so that if our system crashes at a certain point we can use the previous simulation output files rather than starting from scratch.</p> <p>Multiple files were created as part of the simulation, overall the simulation ran for 12 nanoseconds at 310 K.</p> <p>Now the batch file syntax looked as follows:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#!/bin/bash
#SBATCH -p compute
#SBATCH -N 1
#SBATCH -n 8
#SBATCH -t 120:00:00
#SBATCH --job-name="water_system"
#SBATCH -o slurm.%j.out
#SBATCH -e slurm.%j.err
#SBATCH --mail-user=email-id
#SBATCH --mail-type=ALL
</span>
<span class="c1">#module load namd-2.14-gcc-10.2.0-blihfvg
# module load openmpi-4.0.5-gcc-10.2.0-vx4yhsi
#module load namd-2.14-aocc-2.3.0-besm5io
#module load openmpi-4.0.5-aocc-2.3.0-kyn74k2
</span>
<span class="n">spack</span> <span class="n">load</span> <span class="n">namd</span> <span class="o">%</span><span class="n">aocc</span>

<span class="c1">#srun namd2 prod26.in&gt;prodout26
</span>
<span class="c1">#charmrun ++n 8 srun namd2 prod26.in &gt; out
</span><span class="n">namd2</span> <span class="o">+</span><span class="n">setcpuaffinity</span> <span class="o">+</span><span class="n">p8</span> <span class="n">eq1</span><span class="p">.</span><span class="ow">in</span> <span class="o">&gt;</span> <span class="n">out1</span>
<span class="n">mv</span> <span class="n">out1</span> <span class="n">output</span>
<span class="n">namd2</span> <span class="o">+</span><span class="n">setcpuaffinity</span> <span class="o">+</span><span class="n">p8</span> <span class="n">eq2</span><span class="p">.</span><span class="ow">in</span> <span class="o">&gt;</span> <span class="n">out2</span>
<span class="n">mv</span> <span class="n">out2</span> <span class="n">output</span>
<span class="n">namd2</span> <span class="o">+</span><span class="n">setcpuaffinity</span> <span class="o">+</span><span class="n">p8</span> <span class="n">eq3</span><span class="p">.</span><span class="ow">in</span> <span class="o">&gt;</span> <span class="n">out3</span>
<span class="n">mv</span> <span class="n">out3</span> <span class="n">output</span>
<span class="n">namd2</span> <span class="o">+</span><span class="n">setcpuaffinity</span> <span class="o">+</span><span class="n">p8</span> <span class="n">eq4</span><span class="p">.</span><span class="ow">in</span> <span class="o">&gt;</span> <span class="n">out4</span>
<span class="n">mv</span> <span class="n">out4</span> <span class="n">output</span>
</code></pre></div></div> <p>In here, first we write the settings that the systems needs to employ these include - number of cores, CPUs, job name (to keep track of job), name of output log file and name of output error file and finally mail (so that one knows when the job starts running and is completed). Following this, NAMD is loaded onto the cluster and finally run. Since the directory we are running on doesn’t have much of storage space, we move the files obtained after a run to a different directory to ensure that the system doesn’t run out of storage place.</p> <h2 id="files-and-further-analysis">Files and Further Analysis</h2> <p>The files for replicating the simulation are available <a href="https://github.com/AmiteshBadkul/water-box">here</a>. Furthermore, I utilized the output trajectory files to obtain the mean square displacement and further the diffusion coefficient, the blog post for the same is available <a href="https://amiteshbadkul.github.io/posts/2022/04/msd/">here</a>.</p> <h2 id="references">References</h2> <ol> <li><a href="http://li.mit.edu/A/Papers/05/Li05-2.8.pdf">Basic molecular dynamics</a></li> <li><a href="http://people.virginia.edu/~lz2n/mse627/Eduardo">Introduction to Atomistic Simulations</a></li> <li><a href="https://static.igem.org/mediawiki/2009/3/3e/Introduction_to_molecular_Dynamics_Simulation.pdf">Introduction to Molecular Dynamics Simulation - Michael P. Allen</a></li> <li><a href="http://fy.chalmers.se/~tfsgw/CompPhys/lectures/MD_LectureNotes_181111.pdf">Molecular Dynamics- Goran Wahnstrom</a></li> </ol>]]></content><author><name></name></author><category term="molecular-dynamics"/><category term="md"/><category term="vmd"/><category term="namd"/><summary type="html"><![CDATA[understanding simple MD simulations]]></summary></entry></feed>