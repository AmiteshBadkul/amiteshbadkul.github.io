<!DOCTYPE html> <html> <head> <meta name="google-site-verification" content="google-site-verification=google5d0dde76b59b2bd0.html"/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Chest X-Ray Classification | Amitesh Badkul</title> <meta name="author" content="Amitesh Badkul"/> <meta name="description" content="AI-based Tool"/> <meta name="keywords" content="deep learning, bioinformatics, machine learning, image processing"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ’­</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://amiteshbadkul.github.io/blog/2022/cxr/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <d-front-matter> <script async type="text/json">{
      "title": "Chest X-Ray Classification",
      "description": "AI-based Tool",
      "published": "June 1, 2022",
      "authors": [
        {
          "author": "Amitesh Badkul",
          "authorURL": "https://amiteshbadkul.github.io",
          "affiliations": [
            {
              "name": "BITS Pilani, Hyderabad",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://amiteshbadkul.github.io/"><span class="font-weight-bold">AmiteshÂ </span>Badkul</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Chest X-Ray Classification</h1> <p>AI-based Tool</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <div><a href="#dataset">Dataset</a></div> <div><a href="#preliminary-work">Preliminary Work</a></div> <div><a href="#bit-plane-slicing">Bit Plane Slicing</a></div> <div><a href="#results-and-discussions">Results and Discussions</a></div> <div><a href="#code">Code</a></div> <div><a href="#references">References</a></div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>To understand the importance of the features learnt while performing transfer learning in case of Deep Convolutional Neural Networks (DCNNs), I selected the problem of classification of Chest X-Rays into COVID-19 positive and negative. I intend to perform bit plane slicing and provide these bit planes separately as input to the DCNNs for transfer learning.</p> <h2 id="dataset">Dataset</h2> <p>The dataset used for the study can be obtained from <a href="https://www.kaggle.com/competitions/csc532" target="_blank" rel="noopener noreferrer">Kaggle</a>. The dataset consists of 676 images and 408 images for training and validation respectively belonging to both positive and negative class. The rest of the dataset, that is bit plane sliced images, are obtained manually as explained in the sections later.</p> <h2 id="preliminary-work">Preliminary Work</h2> <p>Initially, the performance of VGG16, InceptionV3, Inception-ResNet, DenseNet121, MobileNetV2, and ResNet101 is measured on the normal dataset to check which model performs the best. The general flow of pre-processing, training and evaluation is described as follows:</p> <ol> <li> <p>Firstly, we perform image augmentation. DCNNs frequently take a substantial amount of training data to attain high efficiency. Image augmentation isÂ a method to increase the performance of DCNNs when creating a robust image classifier with very little training data. Image augmentation generates training images artificially using various processing methods or a mix of techniques, such as random rotation, shifts, shear, flips, etc. Earlier, the augmented images were saved along side the original dataset and provided to the DCNNs for training, however this caused memory constraints, therefore tensorflow introduced the concept of real-time data augmentation as the model trains, which is implemented in this study. This is done for both the training and the validation datasets.</p> </li> <li>Once pre-processing is complete, we begin to define the hyperparameters for training the model. The batch size refers to number of sample data points given to the model before itâ€™s weights are updated. The batch size is defined as 32 because a higher batch size leads to degradation in the quality of the model as observed by its ability to generalize, whereas a lower batch size would increase the training time. Conceptually, we can think of the learning rate of our model as the step size. A learning rate of 0.0001 as it is the standard learning rate adapted by the community for transfer learning problems. Finally, we define the epochs as 50, which refers to number of times the model is trained on the entire dataset. <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="n">Batch</span> <span class="n">Size</span>      <span class="mi">32</span>
<span class="n">Learning</span> <span class="n">Rate</span>   <span class="mf">0.0001</span>
<span class="n">Epochs</span>          <span class="mi">50</span>
</code></pre></div> </div> </li> <li>Lastly, the training of the model begins. Certain criterion are set for training, which include - early stopping, a condition where if a specified metric fails to improve for the duration of a certain number of epochs (defined as patience), the training stops. Reduction of learning rate - a condition where the learning rate is reduced to ensure better training when for a specified number of epochs a certain metric has stopped improving. The metric observed is validation accuracy to avoid the problem of overfitting. Along with this the training and validation accuracy and loss are saved to a csv file for future visualization. <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="n">EarlyStopping</span> <span class="n">Patience</span>      <span class="mi">3</span>
<span class="n">ReductionOfLR</span> <span class="n">Patience</span>      <span class="mi">2</span>
</code></pre></div> </div> </li> </ol> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/ZLfvswZ.png"><br> <i> Accuracy and Validation Accuracy compared for the various models </i> </p> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/tL7si9Q.png"><br> <i> Loss and Validation Loss compared for the various models </i> </p> <p>Observations from the graphs:</p> <ol> <li> <p>VGG16: The VGG16 model is unable to improve itâ€™s validation accuracy for three epochs because it potentially fails to learn vitals features that help distinguish the COVID-19 positive and negative. As seen in the graph the training stops after three epochs and within those iterations it is evident that the model overfits on the training data as it is unable to perform adequately on the validation dataset.</p> </li> <li> <p>InceptionV3, Inception-ResNet, DenseNet121, and ResNet101: The InceptionV3 model has a large variance when validating on unseen data. Other models like Inception-ResNet, DenseNet121, and ResNet101 also have a large difference in the training and validation accuracies and losses.</p> </li> <li> <p>MobileNetV2: The MobileNetV2 model performs the best among the others. It has the highest training accuracy of about 95% and a validation accuracy of about 91%, while this still indicates overfitting, the extend of overfitting is least in this case. The same holds for losses, the difference in the training and validation loss is the least.</p> </li> </ol> <p>In summary, the MobileNet baseline model exhibited promising performance, with increasing accuracy and decreasing loss as training progressed. The validation accuracy and loss further demonstrated the modelâ€™s capability to generalize effectively. These results emphasize the suitability of the MobileNet architecture for the given task and dataset, and indicate the potential for further optimization or fine-tuning to enhance performance therefore now first bit plane slicing is performed and then thereafter train the MobileNetV2 model on the various extracted bit planes.</p> <h2 id="bit-plane-slicing">Bit Plane Slicing</h2> <p>Pixels are digital numbers that are comprised of bits. Instead of emphasizing the gray-level range, we choose to observe each bitâ€™s contribution. It can be done using bit plane slicing. By isolating particular bits of the pixel values in an image, we can often highlight interesting aspects of that image. Higher-order bits usually contain most of the important visual information. Lower-order bits have subtle details.</p> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/f0zPK0K.png"><br> <i> Bit Plane Slicing for an 8 bit image </i> </p> <p>The image below shows the different planes obtained for a CXR, as we can observe the lower bit plane images are not visually informative, whereas the higher ones contain significant information. The lower bit planes have information that is not visually interpretable, but is identified by the DCNN as we will see in the sections ahead.</p> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/6Cr8c0V.png"><br> <i> Different Planes for a CXR </i> </p> <h2 id="results-and-discussions">Results and Discussions</h2> <p>Now that weâ€™ve established the fact MobileNetV2 performs the best, we train the MobileNetV2 model on the bit plane sliced images as shown above.</p> <ul> <li>Bit Plane 0: We observe that the only keeping the least significant bit from each pixel, cause the model to overfit on the data as there isnâ€™t enough visual information to learn from. The train accuracy is about 0.94 and the validation accuracy is appropriately 0.84, which validates the hypothesis of that the model overfits the data. Even when considering the loss the validation loss is almost twice the training loss.</li> </ul> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/vQghlgk.png"><br> <i> Bit Plane 0 (Training and Validation Accuracy and Loss) </i> </p> <ul> <li>Bit Plane 1: An interesting observation regarding this model is that is runs only for 3 epochs, we again implies that the lack of information from this bit plane affects the training of the model significantly as compared to the bit plane 0. The validation accuracy, decreases, and is lesser as compared to the training accuracy. The validation loss is greater than the training loss. Hence, here as well the model overfits on the data due to lack of visual information.</li> </ul> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/bMHei5f.png"><br> <i> Bit Plane 1 (Training and Validation Accuracy and Loss) </i> </p> <ul> <li>Bit Plane 2: The training done for bit plane 2 is quite peculiar and odd as it reaches an accuracy of 100% however we know it is not possible to attain such great accuracy in three epochs and with less data. I have not been able to come up with an explanation as to why this problem occurs, even after trying to re-train the data after extracting the data one more time, the error repeats itself.</li> </ul> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/qjmI568.png"><br> <i> Bit Plane 2 (Training and Validation Accuracy and Loss) </i> </p> <ul> <li>Bit Plane 3: Bit Plane 3 has results similar to Bit Plane 0.</li> </ul> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/iG3r4At.png"><br> <i> Bit Plane 3 (Training and Validation Accuracy and Loss) </i> </p> <ul> <li>Bit Plane 4: As seen by the graph the gap between the validation accuracy and training accuracy decreases, implying that as bit planes increase the level of overfitting decreases. The training and validation loss also follow this trend.</li> </ul> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/oCsJHTM.png"><br> <i> Bit Plane 4 (Training and Validation Accuracy and Loss) </i> </p> <ul> <li>Bit Plane 5: Even though the bit plane 5 model trains for three epochs and overfits on the data, it performs well.</li> </ul> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/mrzMNCK.png"><br> <i> Bit Plane 5 (Training and Validation Accuracy and Loss) </i> </p> <ul> <li>Bit Plane 6 &amp; 7: Both these bit planes overfit on the data, they show patterns similar to the the first two bit planes, however the validation and training losses are quite high in these models. This potentially be because of lack of information.</li> </ul> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/Yddw4me.png"><br> <i> Bit Plane 6 (Training and Validation Accuracy and Loss) </i> </p> <p align="center" width="100%"> <img width="75%" src="https://i.imgur.com/vjJQNhK.png"><br> <i> Bit Plane 7 (Training and Validation Accuracy and Loss) </i> </p> <p>It would be fair to conclude that individual planes when provided as input to the DCNN overfits on the data. Hence training the entire image without any extraction would be a useful tool, fortunately a lot of work has been many works on the same, listed below in the references section.</p> <h2 id="code">Code</h2> <p>The code is written in Python 3, and run on a jupyter notebook, which can be accessed here - <a href="https://github.com/AmiteshBadkul/cxr-bit-plane/tree/master/code" target="_blank" rel="noopener noreferrer">CXR COVID</a>.</p> <h2 id="references">References</h2> <ol> <li><a href="https://link.springer.com/article/10.1007/s10489-020-01829-7" target="_blank" rel="noopener noreferrer">Classification of COVID-19 in chest X-ray images using DeTraC deep convolutional neural network</a></li> <li><a href="https://www.sciencedirect.com/science/article/pii/S0169260720309664" target="_blank" rel="noopener noreferrer">COVID-19 identification in chest X-ray images on flat and hierarchical classification scenarios</a></li> <li><a href="https://www.sciencedirect.com/science/article/pii/S0010482520301736" target="_blank" rel="noopener noreferrer">COVID-19 detection using deep learning models to exploit Social Mimic Optimization and structured chest X-ray images using fuzzy color and stacking approaches</a></li> <li><a href="https://www.sciencedirect.com/science/article/pii/S0010482520301621" target="_blank" rel="noopener noreferrer">Automated detection of COVID-19 cases using deep neural networks with X-ray images</a></li> <li><a href="COVID-Net:%20a%20tailored%20deep%20convolutional%20neural%20network%20design%20for%20detection%20of%20COVID-19%20cases%20from%20chest%20X-ray%20images">COVID-Net: a tailored deep convolutional neural network design for detection of COVID-19 cases from chest X-ray images</a></li> </ol> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <footer class="fixed-bottom"> <div class="container mt-0"> Â© Copyright 2025 Amitesh Badkul. </div> </footer> <d-bibliography src="/assets/bibliography/"></d-bibliography> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>